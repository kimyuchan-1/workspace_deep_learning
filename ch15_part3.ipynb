{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFf3hWFcIu7t",
        "tags": []
      },
      "source": [
        "# 머신 러닝 교과서 - 파이토치편"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxbQJLUzIu7w"
      },
      "source": [
        "<table align=\"left\"><tr><td>\n",
        "<a href=\"https://colab.research.google.com/github/rickiepark/ml-with-pytorch/blob/main/ch15/ch15_part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"코랩에서 실행하기\"/></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eau3ABn3Iu7y",
        "outputId": "99a27dc8-2688-4814-a9eb-3460330a4d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] torch 1.9.0+cpu\n"
          ]
        }
      ],
      "source": [
        "from python_environment_check import check_packages\n",
        "\n",
        "\n",
        "d = {\n",
        "    'torch': '1.8.0',\n",
        "}\n",
        "check_packages(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Yxf5TOIu7z"
      },
      "source": [
        "# 15장 - 순환 신경망으로 순차 데이터 모델링 (파트 3/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOznh67rIu70"
      },
      "source": [
        "**목차**\n",
        "\n",
        "- 파이토치로 시퀀스 모델링을 위한 RNN 구현\n",
        "  - 두 번째 프로젝트: 텐서플로로 글자 단위 언어 모델 구현\n",
        "    - 데이터셋 전처리\n",
        "    - 문자 수준의 RNN 모델 만들기\n",
        "    - 평가 단계: 새로운 텍스트 생성\n",
        "- 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jL7BFbKTIu70"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "탠서플로우 방식:\n",
        "\n",
        " - TensorFlow(Keras) 에서는 nn.Module + forward() 대신\n",
        "\n",
        " - tf.keras.Model + call() 또는 Sequential API를 주로 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (2.20.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (2.3.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (6.33.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (2.32.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (80.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (2.20.0)\n",
            "Requirement already satisfied: keras>=3.10.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: pillow in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
            "Requirement already satisfied: namex in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (8.7.1)\n",
            "Requirement already satisfied: zipp>=3.20 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.20.0->tensorflow) (3.23.0)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\user\\miniconda3\\envs\\torchlightning\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# ----------------------------\n",
        "# 하이퍼파라미터\n",
        "# ----------------------------\n",
        "vocab_size = 10000\n",
        "embed_dim = 128\n",
        "rnn_hidden_size = 64\n",
        "max_len = 100\n",
        "\n",
        "# ----------------------------\n",
        "# 모델 정의 (Sequential)\n",
        "# ----------------------------\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embed_dim,\n",
        "        input_length=max_len,\n",
        "        mask_zero=True   # padding 자동 마스킹 (중요!)\n",
        "    ),\n",
        "    layers.LSTM(rnn_hidden_size),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# 컴파일\n",
        "# ----------------------------\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D92PkQtIu70"
      },
      "source": [
        "## 15.3.2 두 번째 프로젝트: 텐서플로로 글자 단위 언어 모델 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "목적: 입력 문서와 비슷한 스타일로 새로운 텍스트를 생성하는 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "OSiX99uoIu70",
        "outputId": "82c3e562-132b-4e0b-9c51-28a914e67b18"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_11.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_11.png', width=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnY_wM0QIu70"
      },
      "source": [
        "### 1) 데이터셋 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1874년 출간한 신비의 섬(The Mysterious Island)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZLETk7qIu71",
        "outputId": "f517b268-0434-4e8b-dd58-dbf993361405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 길이: 1112350\n",
            "고유한 문자: 80\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "## 텍스트 읽고 전처리하기\n",
        "with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "\n",
        "text = text[start_indx:end_indx]\n",
        "char_set = set(text)\n",
        "print('전체 길이:', len(text))\n",
        "print('고유한 문자:', len(char_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by Anthony '"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:50]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "JZx2RoOyIu71",
        "outputId": "826f72ce-c971-410a-ecff-346fd478c8ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_12.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_12.png', width=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvdS_TvqIu71",
        "outputId": "afb3ed71-311c-4d0a-90f1-e7ba4cfa24ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "인코딩된 텍스트 크기:  (1112350,)\n",
            "THE MYSTERIOUS       == 인코딩 ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
            "[33 43 36 25 38 28]  == 디코딩  ==>  ISLAND\n"
          ]
        }
      ],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)} # 문자 → 정수 인덱스 딕셔너리를 생성\n",
        "char_array = np.array(chars_sorted)\n",
        "\n",
        "text_encoded = np.array( # 각 문자를 char2int를 이용해 정수 인덱스로 변환\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32)\n",
        "\n",
        "print('인코딩된 텍스트 크기: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '     == 인코딩 ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == 디코딩  ==> ', ''.join(char_array[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uOdvroDIu71",
        "outputId": "279d139a-7909-400a-8768-8b44f626ab90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44 -> T\n",
            "32 -> H\n",
            "29 -> E\n",
            "1 ->  \n",
            "37 -> M\n"
          ]
        }
      ],
      "source": [
        "for ex in text_encoded[:5]:\n",
        "    print('{} -> {}'.format(ex, char_array[ex]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "그림 15-13: 목표\n",
        "\n",
        "- 새로운 텍스트를 생성하기 위해 입력 시퀀스가 주어졌을 때 다음 문자를 예측하는 모델\n",
        "\n",
        " > 예측 모델: 80개의 문자를 예측이므로 다중 분류 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "jsOxuLoYIu71",
        "outputId": "22b3a5d5-0548-410f-a815-ac8428ba514a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_13.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_13.png', width=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "파이토치 텍스트 생성 모델:\n",
        "\n",
        "- sequence 길이 = 40\n",
        "\n",
        " > 시퀀스 길이가 길면 더 의미있는 문장 생성\n",
        "\n",
        " > short 시퀀스:  문맥 무시하고 개별 단어를 정확히 감지\n",
        "\n",
        " > 적절한 길이의 시퀀스 길이 찾는 것: 하이퍼파라미터 최적화 문제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "JQDOhNrfIu72",
        "outputId": "4d3d87d8-db34-492f-9d39-cab7ab61e44f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_14.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_14.png', width=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "입력 시퀀스: 처음 40개 문자\n",
        "\n",
        "target 시퀀스:1개 밀린 다음 40개 문자\n",
        "\n",
        " X = [c0, c1, c2, ..., c39]\n",
        " \n",
        " Y = [c1, c2, c3, ..., c40]\n",
        "\n",
        "각 시점마다 다음 문자를 예측\n",
        "\n",
        " -> RNN에서 흔히 쓰는 many-to-many + teacher forcing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hOjeE54Iu72",
        "outputId": "17f26668-090b-4a05-ee7d-1908b454484c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
            "  6  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51]  ->  74\n",
            "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'  ->  'y'\n"
          ]
        }
      ],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1 # 41개 문자\n",
        "# chunk 의미: 연속된 시퀀스 조각 > 연속된 부분 문자열\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size+1)]\n",
        "# text_encoded 전체를 한 글자씩 이동(sliding) 하면서 길이 41짜리 조각을 전부 생성\n",
        "## 조사:\n",
        "for seq in text_chunks[:1]: # 전체 데이터 중 첫 번째 chunk 하나만 확인 디버깅 / 이해용\n",
        "    input_seq = seq[:seq_length] # chunk 앞 40개 문자 모델의 입력 X\n",
        "    target = seq[seq_length] # chunk 앞 40개 문자 모델의 입력 X\n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(''.join(char_array[input_seq])),\n",
        "          ' -> ', repr(''.join(char_array[target])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "현재 코드 (many-to-one)\n",
        "\n",
        " - 출력은 마지막 1개, loss 1개, 구현이 단순"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```Python\n",
        "__getitem__()은 파이썬 객체가 “인덱싱 대상”처럼 보일 때 자동으로 호출되는 핵심 메커니즘\n",
        "\n",
        "1) obj[key]\n",
        "\n",
        " > obj.__getitem__(key)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__getitem__ called with 3\n",
            "30\n",
            "__call__ called with 3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 파이썬 객체 호출 규약(protocol)\n",
        "class A:\n",
        "    def __getitem__(self, idx):\n",
        "        print(\"__getitem__ called with\", idx)\n",
        "        return idx * 10\n",
        "\n",
        "    def __call__(self, x):\n",
        "        print(\"__call__ called with\", x)\n",
        "        return x * 100\n",
        "\n",
        "a = A()\n",
        "print(a[3]) # a.__getitem__(3) 으로 자동 변환\n",
        "\n",
        "a(3) # a.__call__(3)으로 자동 변환\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```Python\n",
        "2) obj[start:stop:step] # 슬라이싱\n",
        "\n",
        " > obj.__getitem__(slice(start, stop, step))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```Python\n",
        "3) for x in obj:\n",
        "    ...\n",
        "\n",
        " > iter(obj) 시도 → __iter__()\n",
        "\n",
        " > 없으면 __getitem__(0), __getitem__(1), ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "class B:\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= 3:\n",
        "            raise IndexError\n",
        "        return idx\n",
        "\n",
        "for x in B():\n",
        "    print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```Python\n",
        "4) x in obj\n",
        "\n",
        " > __contains__()가 없으면\n",
        "\n",
        " > 반복 시도 → __getitem__() 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class C:\n",
        "    def __getitem__(self, idx):\n",
        "        if idx == 0: return 'a'\n",
        "        if idx == 1: return 'b'\n",
        "        raise IndexError\n",
        "\n",
        "'a' in C()   # True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```Python\n",
        "5) PyTorch Dataset\n",
        "\n",
        "  dataset[i]\n",
        "\n",
        "  > dataset.__getitem__(i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.26.4\n",
            "1.9.0+cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "print(np.__version__)\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting numpy<2\n",
            "  Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
            "Installing collected packages: numpy\n",
            "Successfully installed numpy-1.26.4\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchmetrics 1.8.2 requires torch>=2.0.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchvision 0.23.0 requires torch==2.8.0, but you have torch 1.9.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "%pip uninstall -y numpy\n",
        "%pip install \"numpy<2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImCA-iHOIu72",
        "outputId": "586e78d4-e21e-461e-9969-dd516bf81e0a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long() # 입력 시퀀스, target 시퀀스를 반환\n",
        "\n",
        "text_chunks_np = np.asarray(text_chunks, dtype=np.int64)   # (N, L) 이어야 함\n",
        "seq_dataset = TextDataset(torch.from_numpy(text_chunks_np))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "enumerate(seq_dataset)\n",
        "\n",
        " > enumerate는 인덱스 번호를 붙여주는 래퍼(wrapper)일 뿐 데이터 접근 방식에는 관여하지 않는다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# enumerate() 하는 일\n",
        "count = 0\n",
        "for element in iterable: # seq_dataset가 iterable 객체이므로 in에서 __getitem__() 호출\n",
        "    yield count, element\n",
        "    count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2iHzdYQIu72",
        "outputId": "83084164-5f74-43a9-de7b-69d97ba568f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "입력 (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
            "타깃 (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
            "\n",
            "입력 (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
            "타깃 (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by '\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset): # TextDataset.__getitem__()이 자동으로 호출\n",
        "    print('입력 (x):', repr(''.join(char_array[seq])))\n",
        "    print('타깃 (y):', repr(''.join(char_array[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uFZVsNHWIu72"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "# device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Jq4KJNKEIu72"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "# 미니 배치로 변환"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSGAxO5Iu73"
      },
      "source": [
        "### 2)  문자 수준의 RNN 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: False\n",
            "PyTorch version: 1.9.0+cpu\n",
            "CUDA version: None\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcqbQphyIu73",
        "outputId": "b07e62f4-f77f-4fe9-fb54-b45e01b90905"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(80, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)\n",
        "\n",
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIQ1KQ7OIu73",
        "outputId": "4f532633-ac98-4b00-dcdf-e24cbf137ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "에포크 0 손실: 4.3706\n",
            "에포크 500 손실: 1.4209\n",
            "에포크 1000 손실: 1.2304\n",
            "에포크 1500 손실: 1.1878\n",
            "에포크 2000 손실: 1.2030\n",
            "에포크 2500 손실: 1.1323\n",
            "에포크 3000 손실: 1.2019\n",
            "에포크 3500 손실: 1.1798\n",
            "에포크 4000 손실: 1.1201\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      9\u001b[0m     hidden, cell \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(batch_size)\n\u001b[1;32m---> 10\u001b[0m     seq_batch, target_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     seq_batch \u001b[38;5;241m=\u001b[39m seq_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     target_batch \u001b[38;5;241m=\u001b[39m target_batch\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 517\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    520\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:556\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 556\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:508\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\utils\\data\\sampler.py:228\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m batch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m--> 228\u001b[0m     \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(idx)\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 시간 소요 30분 이상 걸림\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 10000\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "        # RNN에는 hidden이 있는데, 왜 출력으로 pred만 쓰고\n",
        "        # hidden은 출력으로 사용하지 않는가?”\n",
        "        # pred → 현재 시점의 문자 확률 분포 (logits)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'에포크 {epoch} 손실: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwasl-_fIu73"
      },
      "source": [
        "### 3)  평가 단계: 새로운 텍스트 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RNN 모델: 80개의 문자에 대한 예측 > 80개의 logits를 반환\n",
        "\n",
        "softmax(xi​)= ​exi​​ /∑​exj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Categorical(logits=z)는 내부적으로 softmax(z)를 적용하여 확률 분포를 만들므로,\n",
        "\n",
        "Categorical(probs=softmax(z))와 같은 확률 분포를 정의한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW8EKpLlIu73",
        "outputId": "877848a9-9b40-49df-de80-7743b939de65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확률: [0.33333334 0.33333334 0.33333334]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 1.0]]) # 상대적인인 크기 > [1.0,1.0,2.0] 시도하기 \n",
        "# 확률로 바꾸기 전의 원시 점수(raw score)\n",
        "print('확률:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits) # Categorical(probs=softmax(logits))\n",
        "samples = m.sample((10,))# 확률에 비례한 다양성\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkDBsUqtIu73",
        "outputId": "c44dbb71-6ecf-4780-f4ac-14749ce95b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "확률: [0.10650698 0.10650698 0.78698605]\n",
            "[[0]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('확률:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits) # softmax로 확률로 바꾼다 \n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "로짓(logits)은 모델이 각 클래스를 얼마나 선호하는지를 나타내는 원시 점수이며,\n",
        "\n",
        "이 점수에 softmax를 적용하면 확률이 된다.\n",
        "```Python\n",
        " - logits = 모델의 마지막 선형 출력\n",
        "\n",
        " - hidden  →  Linear(Wx + b)  →  logits  →  Softmax  →  확률\n",
        "\n",
        "  > 통계학의 logit(log-odds) 개념에서 유래: logit = log(p/1-p)\n",
        "\n",
        "  > odds = p / (1-p) > p = 0.8이면 odds = 0.8/(1-0.8) = 4 (양성이 음성보다 4배)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "언어 모델의 기본 가정 (가장 중요)\n",
        "\n",
        " - 문자 언어 모델은 다음을 학습.\n",
        "\n",
        "𝑃(𝑐𝑡∣𝑐1,𝑐2,...,𝑐𝑡−1) # 언어 모델은 다음 1개 문자(토큰) 예측 문제이다\n",
        "\n",
        " > “지금까지 본 문자들이 주어졌을 때, 다음 문자가 무엇일까?”\n",
        "\n",
        " > many-to-many 학습 방식을 구현 방식으로 채택"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for c in range(seq_length):\n",
        "    pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "    loss += loss_fn(pred, target_batch[:, c])\n",
        "    # 각 시점마다 다음 1개 문자를 예측하고 loss를 누적”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "P(c2|c1)\n",
        "P(c3|c1,c2)\n",
        "P(c4|c1,c2,c3)\n",
        "P(c5|c1,c2,c3,c4)\n",
        "\n",
        "- 다음 1개 문자 예측”을 여러 시점에서 동시에 학습\n",
        "\n",
        "- RNN: many-to-many > 여러시점에서 출력이 있다\n",
        "\n",
        " > 예측 단위: 항상 다음 토큰\n",
        "\n",
        " > 훈련: many-to-many\n",
        "\n",
        " > 생성: one-step autoregressive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(지금까지의 문맥) + (현재 입력)\n",
        "            ↓\n",
        "      다음 토큰 예측\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwFFj1HKIu73",
        "outputId": "060d4f97-53a0-4b51-b45a-b65c1e21ab39"
      },
      "outputs": [],
      "source": [
        "# 문자 단위 RNN/LSTM 언어 모델로 텍스트를 생성(sample)\n",
        "def sample(model, starting_str, # 모델, 생성의 시작 문자열\n",
        "           len_generated_text=500, # 새로 생성할 문자 수\n",
        "           scale_factor=1.0): # 로짓 스케일링\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str]) # 시작 문자열의 각 문자를 정수 인덱스로 변환\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1)) # (1, len(starting_str))\n",
        "\n",
        "    generated_str = starting_str\n",
        "    # 시작 문자열 = \"hell\"\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1) # RNN/LSTM의 초기 은닉 상태\n",
        "    hidden = hidden.to('cpu')\n",
        "    cell = cell.to('cpu')\n",
        "    # hidden 상태를 워밍업/맞추기 작업 > \"hel\"\n",
        "    for c in range(len(starting_str)-1): #시작 문자열의 마지막 글자 전까지 \n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) # 한 글자씩 모델에 입력 > 출력 버리고, hidden, cell만 갱신\n",
        "\n",
        "    last_char = encoded_input[:, -1] # 마지막 문자를 따로 저장 > 'l'\n",
        "    for i in range(len_generated_text): # 새로 생성할 문자 수 만큼 반복\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell) # 지금까지 나온 마지막 문자”를 입력으로 주어\n",
        "        ## 그 다음에 나올 문자”의 확률을 예측하기 위해서\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits) # 로짓 → softmax → 확률 분포\n",
        "        # 다음 문자를 뽑기 위한 확률 모델\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to('cpu')\n",
        "print(sample(model, starting_str='The island'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) warm-up\n",
        "for c in range(len(starting_str) - 1):\n",
        "    _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "# 2) generate\n",
        "last_char = encoded_input[:, -1]\n",
        "for _ in range(len_generated_text):\n",
        "    logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "    last_char = Categorical(logits=logits.squeeze(0) * scale_factor).sample()\n",
        "    generated_str += char_array[last_char]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "\n",
        "- 입력: 현재까지 생성된 문자열의 마지막 글자\n",
        "\n",
        "- 목적: 다음 글자 확률(logits) 얻기\n",
        "\n",
        "- 출력(logits): 반드시 사용 (샘플링)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모든 생성형 AI(GPT 포함)의 가장 핵심 원리: \n",
        "\n",
        "- 3가지 생성형 AI의 필수 조건을 모두 만족:\n",
        "\n",
        "1) 확률 모델\n",
        "\n",
        "  > m = Categorical(logits=scaled_logits)\n",
        "\n",
        "    last_char = m.sample()\n",
        "\n",
        "2) 자기 출력(self-output)을 다시 입력으로 사용한다\n",
        "\n",
        " > last_char → model → next_char → 다시 last_char\n",
        "\n",
        "   >> 모델이 스스로 생성한 결과를 다음 입력으로 사용\n",
        "\n",
        "   >> 외부 정답 사용않는다 ⇒ “자기 주도적 생성”\n",
        "\n",
        "3) 열린 길이(open-ended generation)\n",
        "\n",
        "   for i in range(len_generated_text):\n",
        "\n",
        "\n",
        "Autoregressive Generative Model (자기회귀 생성 모델)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "생성형 AI의 정의:\n",
        "\n",
        "1) 세계(언어)를 확률 분포로 학습\n",
        "\n",
        "2) 그 분포에서 자율적으로 샘플\n",
        "\n",
        "3) 샘플 결과를 다시 입력\n",
        "\n",
        "4) 의미 있는 시퀀스를 만들어냄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 코드는 단순한 RNN 예제가 아니라,\n",
        "\n",
        "확률·자기회귀·샘플링이라는 생성형 AI의 핵심 원리를\n",
        "\n",
        "가장 작고 명확한 형태로 구현한 ‘미니 GPT’."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lecW4dFUIu74"
      },
      "source": [
        "* **예측 가능성 vs. 무작위성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki61mzaTIu74",
        "outputId": "adc3190b-d55f-47df-c3c4-c2ccf7b3f28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "스케일 조정 전의 확률:         [0.10650698 0.10650698 0.78698605]\n",
            "0.5배 조정 후 확률: [0.21194156 0.21194156 0.57611686]\n",
            "0.1배 조정 후 확률: [0.3104238  0.3104238  0.37915248]\n"
          ]
        }
      ],
      "source": [
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('스케일 조정 전의 확률:        ', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "print('0.5배 조정 후 확률:', nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n",
        "\n",
        "print('0.1배 조정 후 확률:', nn.functional.softmax(0.1*logits, dim=1).numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-qdDePVIu74",
        "outputId": "30e116e3-b9cf-4844-95fa-e0f88d81783b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island was an\n",
            "insure, and the sand would be seen their close of the house, and the intelligence and a straight of the surrounded on the shores of the island would have an abundant which would have been able to followed by the power.\n",
            "\n",
            "“No, my friend,” said Herbert, “and as if you, Pencroft and Herbert was that the first rivales were completed the operation of the colonists and his companions.\n",
            "\n",
            "Herbert, who was seen the colonists’ pottering of the coast of the colonists were honest for the convicts, the\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island',\n",
        "             scale_factor=2.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LszAuKeYIu74",
        "outputId": "2de6ccc0-d8a6-428a-868e-9c5336d6b0c1",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island\n",
            "would bozent jemutterinuana rollum. “Lazzing ISLCalson of anihmic; Cape, I pier\n",
            "‘pinutnely,” Tide,” said Coolleguark: their arras; t miys by--nisaries,” he akyed Cyrtam,\n",
            "smple;\n",
            "to hain; in--nder, Cor exhaoial;, an inhago was deed--a hruck frosts, Jacriam clvinomes’, at lo,-koednoated, grar, he escen;\n",
            "Positm ton ”ickally\n",
            "severally drawn I am-Neb “or here, pierced how ly histe’s privatebk, we diok recemevin,\n",
            "leanging?” sighted, two\n",
            "CPratch fallewled! Yon.\n",
            "\n",
            "After an\n",
            "weakle nearl fifly Lur, will we\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island',\n",
        "             scale_factor=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHjUA38pIu74"
      },
      "source": [
        "# 요약"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torchlightning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
