{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFf3hWFcIu7t",
        "tags": []
      },
      "source": [
        "# ë¨¸ì‹  ëŸ¬ë‹ êµê³¼ì„œ - íŒŒì´í† ì¹˜í¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxbQJLUzIu7w"
      },
      "source": [
        "<table align=\"left\"><tr><td>\n",
        "<a href=\"https://colab.research.google.com/github/rickiepark/ml-with-pytorch/blob/main/ch15/ch15_part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"ì½”ë©ì—ì„œ ì‹¤í–‰í•˜ê¸°\"/></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2jS-agIIu7x"
      },
      "source": [
        "## íŒ¨í‚¤ì§€ ë²„ì „ ì²´í¬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUmm5b3Iu7x"
      },
      "source": [
        "check_packages.py ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë¡œë“œí•˜ê¸° ìœ„í•´ í´ë”ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCo8gOexIu7x",
        "outputId": "863d5a0e-bc8a-45d0-a2e7-52170f12572b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-09-05 08:00:39--  https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/python_environment_check.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1629 (1.6K) [text/plain]\n",
            "Saving to: â€˜python_environment_check.pyâ€™\n",
            "\n",
            "\r          python_en   0%[                    ]       0  --.-KB/s               \rpython_environment_ 100%[===================>]   1.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-05 08:00:39 (28.6 MB/s) - â€˜python_environment_check.pyâ€™ saved [1629/1629]\n",
            "\n",
            "--2023-09-05 08:00:40--  https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/1268-0.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1171600 (1.1M) [text/plain]\n",
            "Saving to: â€˜1268-0.txtâ€™\n",
            "\n",
            "1268-0.txt          100%[===================>]   1.12M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-09-05 08:00:46 (55.2 MB/s) - â€˜1268-0.txtâ€™ saved [1171600/1171600]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# ì½”ë©ì˜ ê²½ìš° ê¹ƒí—ˆë¸Œ ì €ì¥ì†Œë¡œë¶€í„° python_environment_check.pyë¥¼ ë‹¤ìš´ë¡œë“œ í•©ë‹ˆë‹¤.\n",
        "if 'google.colab' in sys.modules:\n",
        "    !wget https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/python_environment_check.py\n",
        "    !wget https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/1268-0.txt\n",
        "else:\n",
        "    sys.path.insert(0, '..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WUpT7C4Iu7y"
      },
      "source": [
        "ê¶Œì¥ íŒ¨í‚¤ì§€ ë²„ì „ì„ í™•ì¸í•˜ì„¸ìš”:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eau3ABn3Iu7y",
        "outputId": "99a27dc8-2688-4814-a9eb-3460330a4d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Your Python version is 3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\venv39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] torch 1.8.1+cpu\n"
          ]
        }
      ],
      "source": [
        "from python_environment_check import check_packages\n",
        "\n",
        "\n",
        "d = {\n",
        "    'torch': '1.8.0',\n",
        "}\n",
        "check_packages(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Yxf5TOIu7z"
      },
      "source": [
        "# 15ì¥ - ìˆœí™˜ ì‹ ê²½ë§ìœ¼ë¡œ ìˆœì°¨ ë°ì´í„° ëª¨ë¸ë§ (íŒŒíŠ¸ 3/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOznh67rIu70"
      },
      "source": [
        "**ëª©ì°¨**\n",
        "\n",
        "- íŒŒì´í† ì¹˜ë¡œ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì„ ìœ„í•œ RNN êµ¬í˜„\n",
        "  - ë‘ ë²ˆì§¸ í”„ë¡œì íŠ¸: í…ì„œí”Œë¡œë¡œ ê¸€ì ë‹¨ìœ„ ì–¸ì–´ ëª¨ë¸ êµ¬í˜„\n",
        "    - ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
        "    - ë¬¸ì ìˆ˜ì¤€ì˜ RNN ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "    - í‰ê°€ ë‹¨ê³„: ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ìƒì„±\n",
        "- ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.60.2-cp39-cp39-win_amd64.whl.metadata (115 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in c:\\venv39\\lib\\site-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\venv39\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\venv39\\lib\\site-packages (from matplotlib) (11.3.0)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\venv39\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Collecting importlib-resources>=3.2.0 (from matplotlib)\n",
            "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\venv39\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.23.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\venv39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl (7.8 MB)\n",
            "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 2.6/7.8 MB 12.6 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 7.6/7.8 MB 20.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 7.8/7.8 MB 19.3 MB/s  0:00:00\n",
            "Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.2-cp39-cp39-win_amd64.whl (1.5 MB)\n",
            "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.5/1.5 MB 27.4 MB/s  0:00:00\n",
            "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
            "Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
            "Downloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
            "Installing collected packages: pyparsing, kiwisolver, importlib-resources, fonttools, cycler, contourpy, matplotlib\n",
            "\n",
            "   ----------- ---------------------------- 2/7 [importlib-resources]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ----------------- ---------------------- 3/7 [fonttools]\n",
            "   ---------------------------- ----------- 5/7 [contourpy]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------- ----- 6/7 [matplotlib]\n",
            "   ---------------------------------------- 7/7 [matplotlib]\n",
            "\n",
            "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.60.2 importlib-resources-6.5.2 kiwisolver-1.4.7 matplotlib-3.9.4 pyparsing-3.3.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jL7BFbKTIu70"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D92PkQtIu70"
      },
      "source": [
        "## 15.3.2 ë‘ ë²ˆì§¸ í”„ë¡œì íŠ¸: í…ì„œí”Œë¡œë¡œ ê¸€ì ë‹¨ìœ„ ì–¸ì–´ ëª¨ë¸ êµ¬í˜„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "OSiX99uoIu70",
        "outputId": "82c3e562-132b-4e0b-9c51-28a914e67b18"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_11.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_11.png', width=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnY_wM0QIu70"
      },
      "source": [
        "### ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1874ë…„ ì¶œê°„í•œ ì‹ ë¹„ì˜ ì„¬(The Mysterious Island)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZLETk7qIu71",
        "outputId": "f517b268-0434-4e8b-dd58-dbf993361405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì „ì²´ ê¸¸ì´: 1112350\n",
            "ê³ ìœ í•œ ë¬¸ì: 80\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "## í…ìŠ¤íŠ¸ ì½ê³  ì „ì²˜ë¦¬í•˜ê¸°\n",
        "with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "\n",
        "text = text[start_indx:end_indx]\n",
        "char_set = set(text)\n",
        "print('ì „ì²´ ê¸¸ì´:', len(text))\n",
        "print('ê³ ìœ í•œ ë¬¸ì:', len(char_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by Anthony '"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text[:50]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "JZx2RoOyIu71",
        "outputId": "826f72ce-c971-410a-ecff-346fd478c8ed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_12.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_12.png', width=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvdS_TvqIu71",
        "outputId": "afb3ed71-311c-4d0a-90f1-e7ba4cfa24ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì¸ì½”ë”©ëœ í…ìŠ¤íŠ¸ í¬ê¸°:  (1112350,)\n",
            "THE MYSTERIOUS       == ì¸ì½”ë”© ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
            "[33 43 36 25 38 28]  == ë””ì½”ë”©  ==>  ISLAND\n"
          ]
        }
      ],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)} # ë¬¸ì â†’ ì •ìˆ˜ ì¸ë±ìŠ¤ ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±\n",
        "char_array = np.array(chars_sorted)\n",
        "\n",
        "text_encoded = np.array( # ê° ë¬¸ìë¥¼ char2intë¥¼ ì´ìš©í•´ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32)\n",
        "\n",
        "print('ì¸ì½”ë”©ëœ í…ìŠ¤íŠ¸ í¬ê¸°: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '     == ì¸ì½”ë”© ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == ë””ì½”ë”©  ==> ', ''.join(char_array[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uOdvroDIu71",
        "outputId": "279d139a-7909-400a-8768-8b44f626ab90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "44 -> T\n",
            "32 -> H\n",
            "29 -> E\n",
            "1 ->  \n",
            "37 -> M\n"
          ]
        }
      ],
      "source": [
        "for ex in text_encoded[:5]:\n",
        "    print('{} -> {}'.format(ex, char_array[ex]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "jsOxuLoYIu71",
        "outputId": "22b3a5d5-0548-410f-a815-ac8428ba514a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_13.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_13.png', width=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "íŒŒì´í† ì¹˜ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸:\n",
        "\n",
        "- sequence ê¸¸ì´ = 40\n",
        "\n",
        " > ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ë©´ ë” ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ ìƒì„±\n",
        "\n",
        " > short ì‹œí€€ìŠ¤:  ë¬¸ë§¥ ë¬´ì‹œí•˜ê³  ê°œë³„ ë‹¨ì–´ë¥¼ ì •í™•íˆ ê°ì§€\n",
        "\n",
        " > ì ì ˆí•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ ì°¾ëŠ” ê²ƒ: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë¬¸ì œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "JQDOhNrfIu72",
        "outputId": "4d3d87d8-db34-492f-9d39-cab7ab61e44f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_14.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_14.png', width=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì…ë ¥ ì‹œí€€ìŠ¤: ì²˜ìŒ 40ê°œ ë¬¸ì\n",
        "\n",
        "target ì‹œí€€ìŠ¤:1ê°œ ë°€ë¦° ë‹¤ìŒ 40ê°œ ë¬¸ì\n",
        "\n",
        " X = [c0, c1, c2, ..., c39]\n",
        " Y = [c1, c2, c3, ..., c40]\n",
        "\n",
        "ê° ì‹œì ë§ˆë‹¤ ë‹¤ìŒ ë¬¸ìë¥¼ ì˜ˆì¸¡\n",
        "\n",
        " -> RNNì—ì„œ í”íˆ ì“°ëŠ” many-to-many + teacher forcing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hOjeE54Iu72",
        "outputId": "17f26668-090b-4a05-ee7d-1908b454484c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
            "  6  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51]  ->  74\n",
            "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'  ->  'y'\n"
          ]
        }
      ],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1 # 41ê°œ ë¬¸ì\n",
        "\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size+1)]\n",
        "# text_encoded ì „ì²´ë¥¼ í•œ ê¸€ìì”© ì´ë™(sliding) í•˜ë©´ì„œ ê¸¸ì´ 41ì§œë¦¬ ì¡°ê°ì„ ì „ë¶€ ìƒì„±\n",
        "## ì¡°ì‚¬:\n",
        "for seq in text_chunks[:1]: # ì „ì²´ ë°ì´í„° ì¤‘ ì²« ë²ˆì§¸ chunk í•˜ë‚˜ë§Œ í™•ì¸ ë””ë²„ê¹… / ì´í•´ìš©\n",
        "    input_seq = seq[:seq_length] # chunk ì• 40ê°œ ë¬¸ì ëª¨ë¸ì˜ ì…ë ¥ X\n",
        "    target = seq[seq_length] # chunk ì• 40ê°œ ë¬¸ì ëª¨ë¸ì˜ ì…ë ¥ X\n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(''.join(char_array[input_seq])),\n",
        "          ' -> ', repr(''.join(char_array[target])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "í˜„ì¬ ì½”ë“œ (many-to-one)\n",
        "\n",
        " - ì¶œë ¥ì€ ë§ˆì§€ë§‰ 1ê°œ, loss 1ê°œ, êµ¬í˜„ì´ ë‹¨ìˆœ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__getitem__()ì€ íŒŒì´ì¬ ê°ì²´ê°€ â€œì¸ë±ì‹± ëŒ€ìƒâ€ì²˜ëŸ¼ ë³´ì¼ ë•Œ ìë™ìœ¼ë¡œ í˜¸ì¶œë˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜\n",
        "\n",
        "1) obj[key]\n",
        "\n",
        " > obj.__getitem__(key)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class A:\n",
        "    def __getitem__(self, idx):\n",
        "        print(\"called\", idx)\n",
        "        return idx * 10\n",
        "\n",
        "a = A()\n",
        "a[3]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2) obj[start:stop:step] # ìŠ¬ë¼ì´ì‹±\n",
        "\n",
        " > obj.__getitem__(slice(start, stop, step))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3) for x in obj:\n",
        "    ...\n",
        "\n",
        " > iter(obj) ì‹œë„ â†’ __iter__()\n",
        "\n",
        " > ì—†ìœ¼ë©´ __getitem__(0), __getitem__(1), ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class B:\n",
        "    def __getitem__(self, idx):\n",
        "        if idx >= 3:\n",
        "            raise IndexError\n",
        "        return idx\n",
        "\n",
        "for x in B():\n",
        "    print(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4) x in obj\n",
        "\n",
        " > __contains__()ê°€ ì—†ìœ¼ë©´\n",
        "\n",
        " > ë°˜ë³µ ì‹œë„ â†’ __getitem__() ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class C:\n",
        "    def __getitem__(self, idx):\n",
        "        if idx == 0: return 'a'\n",
        "        if idx == 1: return 'b'\n",
        "        raise IndexError\n",
        "\n",
        "'a' in C()   # True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5) PyTorch Dataset\n",
        "\n",
        "  dataset[i]\n",
        "\n",
        "  > dataset.__getitem__(i)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImCA-iHOIu72",
        "outputId": "586e78d4-e21e-461e-9969-dd516bf81e0a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long() # ì…ë ¥ ì‹œí€€ìŠ¤, target ì‹œí€€ìŠ¤ë¥¼ ë°˜í™˜\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "enumerate(seq_dataset)\n",
        "\n",
        " > enumerateëŠ” ì¸ë±ìŠ¤ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ì£¼ëŠ” ë˜í¼(wrapper)ì¼ ë¿ ë°ì´í„° ì ‘ê·¼ ë°©ì‹ì—ëŠ” ê´€ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# enumerate() í•˜ëŠ” ì¼\n",
        "count = 0\n",
        "for element in iterable: # seq_datasetê°€ iterable ê°ì²´ì´ë¯€ë¡œ inì—ì„œ __getitem__() í˜¸ì¶œ\n",
        "    yield count, element\n",
        "    count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2iHzdYQIu72",
        "outputId": "83084164-5f74-43a9-de7b-69d97ba568f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì…ë ¥ (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
            "íƒ€ê¹ƒ (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
            "\n",
            "ì…ë ¥ (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
            "íƒ€ê¹ƒ (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by '\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset): # TextDataset.__getitem__()ì´ ìë™ìœ¼ë¡œ í˜¸ì¶œ\n",
        "    print('ì…ë ¥ (x):', repr(''.join(char_array[seq])))\n",
        "    print('íƒ€ê¹ƒ (y):', repr(''.join(char_array[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uFZVsNHWIu72"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "# device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq4KJNKEIu72"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "# ë¯¸ë‹ˆ ë°°ì¹˜ë¡œ ë³€í™˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSGAxO5Iu73"
      },
      "source": [
        "### ë¬¸ì ìˆ˜ì¤€ì˜ RNN ëª¨ë¸ ë§Œë“¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: False\n",
            "PyTorch version: 1.8.1+cpu\n",
            "CUDA version: None\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA version:\", torch.version.cuda)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcqbQphyIu73",
        "outputId": "b07e62f4-f77f-4fe9-fb54-b45e01b90905"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(80, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)\n",
        "\n",
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIQ1KQ7OIu73",
        "outputId": "4f532633-ac98-4b00-dcdf-e24cbf137ab7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì—í¬í¬ 0 ì†ì‹¤: 4.3706\n",
            "ì—í¬í¬ 500 ì†ì‹¤: 1.4209\n",
            "ì—í¬í¬ 1000 ì†ì‹¤: 1.2304\n",
            "ì—í¬í¬ 1500 ì†ì‹¤: 1.1878\n",
            "ì—í¬í¬ 2000 ì†ì‹¤: 1.2030\n",
            "ì—í¬í¬ 2500 ì†ì‹¤: 1.1323\n",
            "ì—í¬í¬ 3000 ì†ì‹¤: 1.2019\n",
            "ì—í¬í¬ 3500 ì†ì‹¤: 1.1798\n",
            "ì—í¬í¬ 4000 ì†ì‹¤: 1.1201\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      9\u001b[0m     hidden, cell \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39minit_hidden(batch_size)\n\u001b[1;32m---> 10\u001b[0m     seq_batch, target_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq_dl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     seq_batch \u001b[38;5;241m=\u001b[39m seq_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m     target_batch \u001b[38;5;241m=\u001b[39m target_batch\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:517\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 517\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    520\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    521\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:556\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 556\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    557\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:508\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\utils\\data\\sampler.py:228\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m batch \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler:\n\u001b[1;32m--> 228\u001b[0m     \u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(idx)\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(batch) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ì‹œê°„ ì†Œìš” 30ë¶„ ì´ìƒ ê±¸ë¦¼\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 10000\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'ì—í¬í¬ {epoch} ì†ì‹¤: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwasl-_fIu73"
      },
      "source": [
        "### í‰ê°€ ë‹¨ê³„: ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ ìƒì„±"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "softmax(xiâ€‹)= â€‹exiâ€‹â€‹ /âˆ‘â€‹exj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Categorical(logits=z)ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ softmax(z)ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥  ë¶„í¬ë¥¼ ë§Œë“¤ë¯€ë¡œ,\n",
        "\n",
        "Categorical(probs=softmax(z))ì™€ ê°™ì€ í™•ë¥  ë¶„í¬ë¥¼ ì •ì˜í•œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW8EKpLlIu73",
        "outputId": "877848a9-9b40-49df-de80-7743b939de65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í™•ë¥ : [0.33333334 0.33333334 0.33333334]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 1.0]]) # ìƒëŒ€ì ì¸ì¸ í¬ê¸° > [1.0,1.0,2.0] ì‹œë„í•˜ê¸° \n",
        "# í™•ë¥ ë¡œ ë°”ê¾¸ê¸° ì „ì˜ ì›ì‹œ ì ìˆ˜(raw score)\n",
        "print('í™•ë¥ :', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits) # Categorical(probs=softmax(logits))\n",
        "samples = m.sample((10,))# í™•ë¥ ì— ë¹„ë¡€í•œ ë‹¤ì–‘ì„±\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkDBsUqtIu73",
        "outputId": "c44dbb71-6ecf-4780-f4ac-14749ce95b88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í™•ë¥ : [0.10650698 0.10650698 0.78698605]\n",
            "[[0]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('í™•ë¥ :', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ë¡œì§“(logits)ì€ ëª¨ë¸ì´ ê° í´ë˜ìŠ¤ë¥¼ ì–¼ë§ˆë‚˜ ì„ í˜¸í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì›ì‹œ ì ìˆ˜ì´ë©°,\n",
        "\n",
        "ì´ ì ìˆ˜ì— softmaxë¥¼ ì ìš©í•˜ë©´ í™•ë¥ ì´ ëœë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ë³¸ ê°€ì • (ê°€ì¥ ì¤‘ìš”)\n",
        "\n",
        " - ë¬¸ì ì–¸ì–´ ëª¨ë¸ì€ ë‹¤ìŒì„ í•™ìŠµ.\n",
        "\n",
        "ğ‘ƒ(ğ‘ğ‘¡âˆ£ğ‘1,ğ‘2,...,ğ‘ğ‘¡âˆ’1)\n",
        "\n",
        " > â€œì§€ê¸ˆê¹Œì§€ ë³¸ ë¬¸ìë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒ ë¬¸ìê°€ ë¬´ì—‡ì¼ê¹Œ?â€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(ì§€ê¸ˆê¹Œì§€ì˜ ë¬¸ë§¥) + (í˜„ì¬ ì…ë ¥)\n",
        "            â†“\n",
        "      ë‹¤ìŒ í† í° ì˜ˆì¸¡\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwFFj1HKIu73",
        "outputId": "060d4f97-53a0-4b51-b45a-b65c1e21ab39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island\n",
            "had been observed thus. But in this iron had reflections, animals of a fire, staircourse you will suppose to think was stranger to wards sedions in this side, and a larbood hands. It is on\n",
            "salt towards the morning, and was abundant,\n",
            "as if they had done molding, thereford diefs penchorathy, Herbert.\n",
            "\n",
            "â€œEuctric,â€ answered Pencroft and his life could have been troubled him, and Pencroft was not been to the works.\n",
            "\n",
            "He sailed at the corners of a fancalic columns of Sucoe. â€œBut what will be long in th\n"
          ]
        }
      ],
      "source": [
        "# ë¬¸ì ë‹¨ìœ„ RNN/LSTM ì–¸ì–´ ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±(sample)\n",
        "def sample(model, starting_str, # ëª¨ë¸, ìƒì„±ì˜ ì‹œì‘ ë¬¸ìì—´\n",
        "           len_generated_text=500, # ìƒˆë¡œ ìƒì„±í•  ë¬¸ì ìˆ˜\n",
        "           scale_factor=1.0): # ë¡œì§“ ìŠ¤ì¼€ì¼ë§\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str]) # ì‹œì‘ ë¬¸ìì—´ì˜ ê° ë¬¸ìë¥¼ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1)) # (1, len(starting_str))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1) # RNN/LSTMì˜ ì´ˆê¸° ì€ë‹‰ ìƒíƒœ\n",
        "    hidden = hidden.to('cpu')\n",
        "    cell = cell.to('cpu')\n",
        "    for c in range(len(starting_str)-1): #ì‹œì‘ ë¬¸ìì—´ì˜ ë§ˆì§€ë§‰ ê¸€ì ì „ê¹Œì§€ \n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell) # í•œ ê¸€ìì”© ëª¨ë¸ì— ì…ë ¥ ì¶œë ¥ ë²„ë¦¬ê³ , hidden, cellë§Œ ê°±ì‹ \n",
        "\n",
        "    last_char = encoded_input[:, -1] # ë§ˆì§€ë§‰ ë¬¸ìë¥¼ ë”°ë¡œ ì €ì¥\n",
        "    for i in range(len_generated_text): # ìƒˆë¡œ ìƒì„±í•  ë¬¸ì ìˆ˜ ë§Œí¼ ë°˜ë³µ\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell) # ì§€ê¸ˆê¹Œì§€ ë‚˜ì˜¨ ë§ˆì§€ë§‰ ë¬¸ìâ€ë¥¼ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´\n",
        "        ## ê·¸ ë‹¤ìŒì— ë‚˜ì˜¬ ë¬¸ìâ€ì˜ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits) # ë¡œì§“ â†’ softmax â†’ í™•ë¥  ë¶„í¬\n",
        "        # ë‹¤ìŒ ë¬¸ìë¥¼ ë½‘ê¸° ìœ„í•œ í™•ë¥  ëª¨ë¸\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to('cpu')\n",
        "print(sample(model, starting_str='The island'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ëª¨ë“  ìƒì„±í˜• AI(GPT í¬í•¨)ì˜ ê°€ì¥ í•µì‹¬ ì›ë¦¬: \n",
        "\n",
        "- 3ê°€ì§€ ìƒì„±í˜• AIì˜ í•„ìˆ˜ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±:\n",
        "\n",
        "1) í™•ë¥  ëª¨ë¸\n",
        "\n",
        "  > m = Categorical(logits=scaled_logits)\n",
        "\n",
        "    last_char = m.sample()\n",
        "\n",
        "2) ìê¸° ì¶œë ¥(self-output)ì„ ë‹¤ì‹œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤\n",
        "\n",
        " > last_char â†’ model â†’ next_char â†’ ë‹¤ì‹œ last_char\n",
        "\n",
        "   >> ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ìƒì„±í•œ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
        "\n",
        "   >> ì™¸ë¶€ ì •ë‹µ ì‚¬ìš©ì•ŠëŠ”ë‹¤ â‡’ â€œìê¸° ì£¼ë„ì  ìƒì„±â€\n",
        "\n",
        "3) ì—´ë¦° ê¸¸ì´(open-ended generation)\n",
        "\n",
        "   for i in range(len_generated_text):\n",
        "\n",
        "\n",
        "Autoregressive Generative Model (ìê¸°íšŒê·€ ìƒì„± ëª¨ë¸)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ìƒì„±í˜• AIì˜ ì •ì˜:\n",
        "\n",
        "1) ì„¸ê³„(ì–¸ì–´)ë¥¼ í™•ë¥  ë¶„í¬ë¡œ í•™ìŠµ\n",
        "\n",
        "2) ê·¸ ë¶„í¬ì—ì„œ ììœ¨ì ìœ¼ë¡œ ìƒ˜í”Œ\n",
        "\n",
        "3) ìƒ˜í”Œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì…ë ¥\n",
        "\n",
        "4) ì˜ë¯¸ ìˆëŠ” ì‹œí€€ìŠ¤ë¥¼ ë§Œë“¤ì–´ëƒ„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì´ ì½”ë“œëŠ” ë‹¨ìˆœí•œ RNN ì˜ˆì œê°€ ì•„ë‹ˆë¼,\n",
        "\n",
        "í™•ë¥ Â·ìê¸°íšŒê·€Â·ìƒ˜í”Œë§ì´ë¼ëŠ” ìƒì„±í˜• AIì˜ í•µì‹¬ ì›ë¦¬ë¥¼\n",
        "\n",
        "ê°€ì¥ ì‘ê³  ëª…í™•í•œ í˜•íƒœë¡œ êµ¬í˜„í•œ â€˜ë¯¸ë‹ˆ GPTâ€™."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lecW4dFUIu74"
      },
      "source": [
        "* **ì˜ˆì¸¡ ê°€ëŠ¥ì„± vs. ë¬´ì‘ìœ„ì„±**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki61mzaTIu74",
        "outputId": "adc3190b-d55f-47df-c3c4-c2ccf7b3f28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ìŠ¤ì¼€ì¼ ì¡°ì • ì „ì˜ í™•ë¥ :         [0.10650698 0.10650698 0.78698605]\n",
            "0.5ë°° ì¡°ì • í›„ í™•ë¥ : [0.21194156 0.21194156 0.57611686]\n",
            "0.1ë°° ì¡°ì • í›„ í™•ë¥ : [0.3104238  0.3104238  0.37915248]\n"
          ]
        }
      ],
      "source": [
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('ìŠ¤ì¼€ì¼ ì¡°ì • ì „ì˜ í™•ë¥ :        ', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "print('0.5ë°° ì¡°ì • í›„ í™•ë¥ :', nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n",
        "\n",
        "print('0.1ë°° ì¡°ì • í›„ í™•ë¥ :', nn.functional.softmax(0.1*logits, dim=1).numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-qdDePVIu74",
        "outputId": "30e116e3-b9cf-4844-95fa-e0f88d81783b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island was an\n",
            "insure, and the sand would be seen their close of the house, and the intelligence and a straight of the surrounded on the shores of the island would have an abundant which would have been able to followed by the power.\n",
            "\n",
            "â€œNo, my friend,â€ said Herbert, â€œand as if you, Pencroft and Herbert was that the first rivales were completed the operation of the colonists and his companions.\n",
            "\n",
            "Herbert, who was seen the colonistsâ€™ pottering of the coast of the colonists were honest for the convicts, the\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island',\n",
        "             scale_factor=2.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LszAuKeYIu74",
        "outputId": "2de6ccc0-d8a6-428a-868e-9c5336d6b0c1",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The island\n",
            "would bozent jemutterinuana rollum. â€œLazzing ISLCalson of anihmic; Cape, I pier\n",
            "â€˜pinutnely,â€ Tide,â€ said Coolleguark: their arras; t miys by--nisaries,â€ he akyed Cyrtam,\n",
            "smple;\n",
            "to hain; in--nder, Cor exhaoial;, an inhago was deed--a hruck frosts, Jacriam clvinomesâ€™, at lo,-koednoated, grar, he escen;\n",
            "Positm ton â€ickally\n",
            "severally drawn I am-Neb â€œor here, pierced how ly histeâ€™s privatebk, we diok recemevin,\n",
            "leanging?â€ sighted, two\n",
            "CPratch fallewled! Yon.\n",
            "\n",
            "After an\n",
            "weakle nearl fifly Lur, will we\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island',\n",
        "             scale_factor=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHjUA38pIu74"
      },
      "source": [
        "# ìš”ì•½"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
