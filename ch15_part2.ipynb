{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCbjd_9Ros6D",
        "tags": []
      },
      "source": [
        "# ë¨¸ì‹  ëŸ¬ë‹ êµê³¼ì„œ - íŒŒì´í† ì¹˜í¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-itZ1LYos6E"
      },
      "source": [
        "<table align=\"left\"><tr><td>\n",
        "<a href=\"https://colab.research.google.com/github/rickiepark/ml-with-pytorch/blob/main/ch15/ch15_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"ì½”ë©ì—ì„œ ì‹¤í–‰í•˜ê¸°\"/></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfyZvq7Wos6H"
      },
      "source": [
        "**ì£¼ì˜**: ì±…ì— ìˆëŠ” ì½”ë“œë¥¼ ì¬í˜„í•˜ë ¤ë©´ ì´ ì¥ì—ì„œ ì‚¬ìš©í•œ íŒ¨í‚¤ì§€ì¸ `torchtext` 0.10.0(https://pypi.org/project/torchtext/0.10.0/) ë²„ì „ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ëŠ” ìµœì‹  ë²„ì „ì˜ `torchtext`ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ëª‡ ê°€ì§€ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHknaWZmos6H"
      },
      "outputs": [],
      "source": [
        "%pip install torchtext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP0YTIOPos6H"
      },
      "source": [
        "ìµœì‹  ë²„ì „ì˜ `torchtext`ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `portalocker`ê°€ í•„ìš”í•©ë‹ˆë‹¤:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-nBse4pos6H",
        "outputId": "92e37694-4809-47dc-9001-7a29d7745db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from portalocker) (311)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-3.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install portalocker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9SrydFRos6G"
      },
      "source": [
        "ê¶Œì¥ íŒ¨í‚¤ì§€ ë²„ì „ì„ í™•ì¸í•˜ì„¸ìš”:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egu2MOufos6G",
        "outputId": "6ed0f73c-f15f-410b-d180-09c8debf3e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] torch 2.9.1+cpu\n",
            "[OK] torchtext 0.6.0\n"
          ]
        }
      ],
      "source": [
        "from python_environment_check import check_packages\n",
        "\n",
        "\n",
        "d = {\n",
        "    'torch': '1.8.0',\n",
        "    'torchtext': '0.6.0'\n",
        "}\n",
        "check_packages(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgX48KNhos6G"
      },
      "source": [
        "# 15ì¥ - ìˆœí™˜ ì‹ ê²½ë§ìœ¼ë¡œ ìˆœì°¨ ë°ì´í„° ëª¨ë¸ë§ (íŒŒíŠ¸ 2/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NcLcT2oos6G"
      },
      "source": [
        "**ëª©ì°¨**\n",
        "\n",
        "- íŒŒì´í† ì¹˜ë¡œ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì„ ìœ„í•œ RNN êµ¬í˜„\n",
        "  - ì²« ë²ˆì§¸ í”„ë¡œì íŠ¸: IMDb ì˜í™” ë¦¬ë·°ì˜ ê°ì„± ë¶„ì„\n",
        "    - ì˜í™” ë¦¬ë·° ë°ì´í„° ì¤€ë¹„\n",
        "    - ë¬¸ì¥ ì¸ì½”ë”©ì„ ìœ„í•œ ì„ë² ë”© ì¸µ\n",
        "    - RNN ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "    - ê°ì„± ë¶„ì„ ì‘ì—…ì„ ìœ„í•œ RNN ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "      - ì–‘ë°©í–¥ RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7jCt27uFos6G"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kbn_pv7os6H"
      },
      "source": [
        "# 15.3 íŒŒì´í† ì¹˜ë¡œ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì„ ìœ„í•œ RNN êµ¬í˜„\n",
        "\n",
        "## 15.3.1 ì²« ë²ˆì§¸ í”„ë¡œì íŠ¸: IMDb ì˜í™” ë¦¬ë·°ì˜ ê°ì„± ë¶„ì„\n",
        "\n",
        "- ê°ì„±ë¶„ì„ : í…ìŠ¤íŠ¸ > ë‹¤ëŒ€ì¼ êµ¬ì¡° RNN\n",
        "\n",
        "### ì˜í™” ë¦¬ë·° ë°ì´í„° ì¤€ë¹„"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LVVfKJDWos6H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLSBYQNmos6I"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "# ë‹¨ê³„ 1: ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤\n",
        "\n",
        "train_dataset = IMDB(split='train') # iterator / iterable dataset\n",
        "test_dataset = IMDB(split='test')\n",
        "\n",
        "# test_dataset = list(test_dataset)   #datapipe to list\n",
        "## iterable ê°ì²´ëŠ” í•œë²ˆë§Œ ìˆœíšŒ > ì—¬ëŸ¬ë²ˆ ìˆœíšŒí•˜ë ¤ë©´ list() ì‚¬ìš©í•´ì•¼ \n",
        "\n",
        "torch.manual_seed(1)\n",
        "# í›ˆë ¨ ë°ì´í„°ë¥¼ í›ˆë ¨, ê²€ì¦ ë°ì´í„°ë¡œ ë¶„í• \n",
        "train_dataset, valid_dataset = random_split(\n",
        "    list(train_dataset), [20000, 5000]) # í›ˆë ¨ 20000ê°œ, ê²€ì¦ 5000ê°œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "IMDB.__init__() missing 3 required positional arguments: 'path', 'text_field', and 'label_field'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m random_split\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1) iterable -> listë¡œ ê³ ì • (ì—¬ëŸ¬ epoch ìœ„í•´)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m train_list = \u001b[38;5;28mlist\u001b[39m(\u001b[43mIMDB\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      7\u001b[39m test_list  = \u001b[38;5;28mlist\u001b[39m(IMDB(root=\u001b[33m\"\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m\"\u001b[39m, split=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mtrain:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_list), \u001b[33m\"\u001b[39m\u001b[33mtest:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(test_list))\n",
            "\u001b[31mTypeError\u001b[39m: IMDB.__init__() missing 3 required positional arguments: 'path', 'text_field', and 'label_field'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.datasets import IMDB\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "# 1) iterable -> listë¡œ ê³ ì • (ì—¬ëŸ¬ epoch ìœ„í•´)\n",
        "train_list = list(IMDB(root=\"./data\", split=\"train\"))\n",
        "test_list  = list(IMDB(root=\"./data\", split=\"test\"))\n",
        "\n",
        "print(\"train:\", len(train_list), \"test:\", len(test_list))\n",
        "print(\"sample:\", train_list[0])  # ë³´í†µ ('pos'/'neg', text) ë˜ëŠ” (label, text)\n",
        "\n",
        "# 2) ë¼ë²¨/í…ìŠ¤íŠ¸ ìˆœì„œ í™•ì¸ í›„ í†µì¼\n",
        "# torchtext IMDBëŠ” ë³´í†µ (label, text) = ('pos','...') í˜•íƒœ\n",
        "# í˜¹ì‹œ ë°˜ëŒ€ë¡œ ë‚˜ì˜¤ë©´ ì•„ë˜ swapë§Œ ë°”ê¾¸ë©´ ë¨.\n",
        "def normalize_example(ex):\n",
        "    label, text = ex  # (label, text) ê°€ì •\n",
        "    if isinstance(label, str):\n",
        "        label = 1 if label.lower() == \"pos\" else 0\n",
        "    return text, label  # (text, label)ë¡œ í†µì¼ (ëª¨ë¸ ì…ë ¥ í¸í•˜ê²Œ)\n",
        "\n",
        "train_list = [normalize_example(ex) for ex in train_list]\n",
        "test_list  = [normalize_example(ex) for ex in test_list]\n",
        "\n",
        "# 3) split í¬ê¸° ì•ˆì „í•˜ê²Œ\n",
        "n_train_total = len(train_list)\n",
        "n_train = 20000\n",
        "n_valid = 5000\n",
        "assert n_train + n_valid <= n_train_total, f\"split í•©ì´ train ê°œìˆ˜({n_train_total})ë³´ë‹¤ í¼\"\n",
        "\n",
        "torch.manual_seed(1)\n",
        "train_dataset, valid_dataset = random_split(train_list, [n_train, n_valid])\n",
        "\n",
        "print(len(train_dataset), len(valid_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('pos',\n",
              "  'An extra is called upon to play a general in a movie about the Russian Revolution. However, he is not any ordinary extra. He is Serguis Alexander, former commanding general of the Russia armies who is now being forced to relive the same scene, which he suffered professional and personal tragedy in, to satisfy the director who was once a revolutionist in Russia and was humiliated by Alexander. It can now be the time for this broken man to finally \"win\" his penultimate battle. This is one powerful movie with meticulous direction by Von Sternberg, providing the greatest irony in Alexander\\'s character in every way he can. Jannings deserved his Oscar for the role with a very moving performance playing the general at his peak and at his deepest valley. Powell lends a sinister support as the revenge minded director and Brent is perfect in her role with her face and movements showing so much expression as Jannings\\' love. All around brilliance. Rating, 10.'),\n",
              " ('pos',\n",
              "  \"almost every review of this movie I'd seen was pretty bad. It's not pretty bad, it's actually pretty good, though not great. The Judy Garland character could have gotten annoying, but she didn't allow it to. Somewhere along the line, i've become a fan of brooding, overbearing, overacting Van Heflin, at least in the early 40's. Judy's singing is great, but the film missed a great chance by not showing more of their relationship. I gave it a 7.\"),\n",
              " ('pos',\n",
              "  'I did not have too much interest in watching The Flock.Andrew Lau co-directed the masterpiece trilogy of Infernal Affairs but he had been fired from The Flock and he had been replaced by an emergency director called Niels Mueller.I had the feeling that Lau had made a good film but it had not satisfied the study,so they fired him and hired another director.This usually does not work well (let\\'s remember The Invasion).But The Flock resulted to be better than what I expected.It\\'s not a great film but it\\'s an interesting and entertaining thriller.The character development is very well done and I could know the characters very well.Also,the relationship between the two main characters is natural and credible.Richard Gere and Claire Danes bring competent performances.Now,let\\'s go to the negative points.One element which really bothered me (there was a moment in which it irritated me) was the excess of edition tricks to give the movie more \"attitude\" and style.That tricks feel out of place and their presence is arbitrary.Plus,I think the film should have been more ambitious.In spite of that,I recommend The Flock as a good thriller.It\\'s not memorable at all,but it\\'s entertaining.'),\n",
              " ('neg',\n",
              "  \"Ulises is a literature teacher that arrives to a coastal town. There, he will fell in love to Martina, the most beautiful girl in town. They will start a torrid romance which will end in the tragic death of Ulises at the sea. Some years later, Martina has married to Sierra, the richest man in town and lives a quiet happy live surrounded by money. One day, the apparition of Ulises will make her passion to rise up and act without thinking the consequences. The plot is quite absurd and none of the actors plays a decent part. IN addition, three quarters of the film are sexual acts, which, still being well filmed, are quite tiring, as we want to see More development of the story. It is just a bad Bigas Luna's film, with lots of sex, no argument and stupid characters everywhere.\"),\n",
              " ('pos',\n",
              "  \"I found The FBI Story considerably entertaining and suitably upbeat for my New Years Day holiday viewing. Its drama and action-packed episodes were thrilling. The Hardesty character was well drawn and admirable. Overall the photography, script and direction was perfectly creditable. Rather than taking the film to be a repugnant piece of propaganda, as some might, I enjoyed it as a well mounted portrayal of the necessity of ingenious minds and brave bodies in the fight against crime. Again, the depiction of a family holding together even under the strain of the husband's commitment to his (arguably) important work, I did not find to be a twee representation but an ideal and exemplary one.\")]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "head5 = [train_dataset[i] for i in range(5)]\n",
        "head5\n",
        "# label, text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PEWHQVoos6I",
        "outputId": "a9bc923a-45c4-4f12-bc3c-5ebf5a9bd9bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì–´íœ˜ ì‚¬ì „ í¬ê¸°: 69023\n"
          ]
        }
      ],
      "source": [
        "## ë‹¨ê³„ 2: ê³ ìœ  í† í° (ë‹¨ì–´) ì°¾ê¸°\n",
        "## IMDB í•™ìŠµ ë°ì´í„°ë¡œë¶€í„° â€œë‹¨ì–´ ë¹ˆë„ ì‚¬ì „(vocabulary statistics)â€ì„ ë§Œë“œëŠ” ì „í˜•ì ì¸ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "token_counts = Counter() # ë‹¨ì–´ë³„ íšŸìˆ˜ ê³„ì‚°, CounterëŠ” dictì˜ í•˜ìœ„ í´ë˜ìŠ¤\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text) #textì—ì„œ <ë¡œ ì‹œì‘í•´ì„œ >ë¡œ ëë‚˜ëŠ” ëª¨ë“  íƒœê·¸ë¥¼ ì œê±°\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower()) # ê°ì„± ì´ëª¨í‹°ì½˜: ëˆˆ, ì½”, ì…\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +' '.join(emoticons).replace('-', '') # íŠ¹ìˆ˜ ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ\n",
        "    # \\W: ì•ŒíŒŒë²³/ìˆ«ì/ì–¸ë”ìŠ¤ì½”ì–´ê°€ ì•„ë‹Œ ë¬¸ì\n",
        "    tokenized = text.split()\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "for label, line in train_dataset:\n",
        "    tokens = tokenizer(line)\n",
        "    token_counts.update(tokens) # í† í° ë¹ˆë„ ìˆ˜\n",
        "\n",
        "\n",
        "print('ì–´íœ˜ ì‚¬ì „ í¬ê¸°:', len(token_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "token_counts = {\n",
        "    \"the\": 231358,\n",
        "    \"movie\": 58923,\n",
        "    \"good\": 41235,\n",
        "    \"bad\": 39821,\n",
        "    ...\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "token_counts.items() > (token, freq) íŠœí”Œë“¤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for token, freq in token_counts.items():\n",
        "    print(token, freq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "movie 12000\n",
        "good 8000\n",
        "bad 7000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkd_ks7Wos6I",
        "outputId": "9f064122-f17b-4a45-d054-777210a72869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11, 7, 35, 457]\n"
          ]
        }
      ],
      "source": [
        "## ë‹¨ê³„ 3: ê³ ìœ  í† í°ì„ ì •ìˆ˜ë¡œ ì¸ì½”ë”©í•˜ê¸°\n",
        "from torchtext.vocab import vocab\n",
        "# items()ëŠ” ë”•ì…”ë„ˆë¦¬(ë˜ëŠ” Counter)ì— ë“¤ì–´ ìˆëŠ” (key, value) ìŒì„ í•œ ë²ˆì— êº¼ë‚´ì£¼ëŠ” í•¨ìˆ˜\n",
        "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True) # ì •ë ¬ì„ ë¹ˆë„ìˆ˜ë¡œ\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples) # ì •ë ¬ëœ íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì„œê°€ ë³´ì¡´ëœ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
        "\n",
        "vocab = vocab(ordered_dict) # ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±\n",
        "\n",
        "vocab.insert_token(\"<pad>\", 0) # íŒ¨ë”© í† í°\n",
        "vocab.insert_token(\"<unk>\", 1)\n",
        "vocab.set_default_index(1)\n",
        "\n",
        "print([vocab[token] for token in ['this', 'is', 'an', 'example']]) # vocab[\"movie\"]ëŠ” ì¸ë±ìŠ¤ë¥¼ ë¦¬í„´\n",
        "# í…ìŠ¤íŠ¸ë¥¼ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¼í›„ LSTM ì…ë ¥ ì „ì²´ íŒŒì´í”„ë¼ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xTOXkt4gos6I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ê²½ê³ : ì´ ì½”ë“œëŠ” CPUì—ì„œ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "if not torch.cuda.is_available():\n",
        "    print(\"ê²½ê³ : ì´ ì½”ë“œëŠ” CPUì—ì„œ ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNNuqvjpos6I"
      },
      "outputs": [],
      "source": [
        "## ë‹¨ê³„ 3-A: ë³€í™˜ í•¨ìˆ˜ ì •ì˜\n",
        "import torchtext\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#xëŠ” í…ìŠ¤íŠ¸\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)] # í† í°ì— ëŒ€í•œ ì •ìˆ˜ ì¸ë±ìŠ¤\n",
        "# í…ìŠ¤íŠ¸ â†’ í† í° â†’ ì¸ë±ìŠ¤ ë³€í™˜ ê·œì¹™ì„ í•˜ë‚˜ì˜ â€˜í•¨ìˆ˜ ê°ì²´â€™ë¡œ ì •ì˜\n",
        "# def text_pipeline(x):\n",
        "#    return [vocab[token] for token in tokenizer(x)]\n",
        "\n",
        "# ì˜ˆ: [\"I\",\"love\",\"it\"] â†’ [12, 534, 87] ê°™ì€ ì •ìˆ˜ ë¦¬ìŠ¤íŠ¸\n",
        "from torchtext import __version__ as torchtext_version\n",
        "from pkg_resources import parse_version\n",
        "\n",
        "if parse_version(torchtext.__version__) > parse_version(\"0.10\"):\n",
        "    label_pipeline = lambda x: 1. if x == 2 else 0.         # 1 ~ ë¶€ì • ë¦¬ë·°, 2 ~ ê¸ì • ë¦¬ë·°\n",
        "else:\n",
        "    label_pipeline = lambda x: 1. if x == 'pos' else 0.\n",
        "# ê¸ì •: 1.0, ë¶€ì •: 0.0\n",
        "\n",
        "## ë‹¨ê³„ 3-B: ì¸ì½”ë”©ê³¼ ë³€í™˜ í•¨ìˆ˜ ê°ì‹¸ê¸°\n",
        "# ê°€ë³€ ê¸¸ì´ í…ìŠ¤íŠ¸ ë°°ì¹˜ë¥¼ ëª¨ë¸ì´ ë°”ë¡œ ë°›ì„ ìˆ˜ ìˆëŠ” í…ì„œ í˜•íƒœë¡œ ë¬¶ì–´ ì£¼ëŠ” í•¨ìˆ˜\n",
        "def collate_batch(batch): #batchëŠ” ë¯¸ë‹ˆë°°ì¹˜ í…ìŠ¤íŠ¸\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for _label, _text in batch: # batch = [(label1, text1), (), ...]\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        # label_pipeline: ì›ë³¸ ë¼ë²¨ â†’ 0.0 / 1.0\n",
        "        processed_text = torch.tensor(text_pipeline(_text),\n",
        "                                      dtype=torch.int64)\n",
        "        # text_pipeline(_text) ë¬¸ìì—´ â†’ í† í°í™” â†’ vocab ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸\n",
        "        ## \"i love this movie\"\n",
        "        ## â†’ [\"i\",\"love\",\"this\",\"movie\"]\n",
        "        ## â†’ [12, 534, 78, 921]\n",
        "\n",
        "        text_list.append(processed_text)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
        "        text_list, batch_first=True)\n",
        "    # ê¸¸ì´ê°€ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ë“¤ì„ ê°™ì€ ê¸¸ì´ë¡œ ë§ì¶¤\n",
        "    # ì§§ì€ ë¬¸ì¥ì€ PAD í† í°(0) ìœ¼ë¡œ ë’¤ë¥¼ ì±„ì›€\n",
        "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9kCgbEOos6I",
        "outputId": "bb3ff339-2f3c-4935-e491-ca4f92005e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n",
            "             4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,\n",
            "           100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,\n",
            "             5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n",
            "         11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n",
            "          1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n",
            "         42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,\n",
            "           148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n",
            "          1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n",
            "         15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,\n",
            "          3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,\n",
            "            27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n",
            "           395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,\n",
            "          5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,\n",
            "           155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,\n",
            "           390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,\n",
            "            31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  216,   175,   724,     5,    11,    18,    10,   226,   110,    14,\n",
            "           182,    78,     8,    13,    24,   182,    78,     8,    13,   166,\n",
            "           182,    50,   150,    24,    85,     2,  4031,  5935,   107,    96,\n",
            "            28,  1867,   602,    19,    52,   162,    21,  1698,     8,     6,\n",
            "          1181,   367,     2,   351,    10,   140,   419,     4,   333,     5,\n",
            "          6022,  7136,  5055,  1209, 10892,    32,   219,     9,     2,   405,\n",
            "          1413,    13,  4031,    13,  1099,     7,    85,    19,     2,    20,\n",
            "          1018,     4,    85,   565,    34,    24,   807,    55,     5,    68,\n",
            "           658,    10,   507,     8,     4,   668,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [   10,   121,    24,    28,    98,    74,   589,     9,   149,     2,\n",
            "          7372,  3030, 14543,  1012,   520,     2,   985,  2327,     5, 16847,\n",
            "          5479,    19,    25,    67,    76,  3478,    38,     2,  7372,     3,\n",
            "            25,    67,    76,  2951,    34,    35, 10893,   155,   449, 29495,\n",
            "         23725,    10,    67,     2,   554,    12, 14543,    67,    91,     4,\n",
            "            50,    20,    19,     8,    67,    24,  4228,     2,  2142,    37,\n",
            "            33,  3478,    87,     3,  2564,   160,   155,    11,   634,   126,\n",
            "            24,   158,    72,   286,    13,   373,     2,  4804,    19,     2,\n",
            "          7372,  6794,     6,    30,   128,    73,    48,    10,   886,     8,\n",
            "            13,    24,     4,    85,    20,    19,     8,    13,    35,   218,\n",
            "             3,   428,   710,     2,   107,   936,     7,    54,    72,   223,\n",
            "             3,    10,    96,   122,     2,   103,    54,    72,    82,     2,\n",
            "           658,   202,     2,   106,   293,   103,     7,  1193,     3,  3031,\n",
            "           708,  5760,     3,  2918,  3991,   706,  3327,   349,   148,   286,\n",
            "            13,   139,     6,     2,  1501,   750,    29,  1407,    62,    65,\n",
            "          2612,    71,    40,    14,     4,   547,     9,    62,     8,  7943,\n",
            "            71,    14,     2,  5687,     5,  4868,  3111,     6,   205,     2,\n",
            "            18,    55,  2075,     3,   403,    12,  3111,   231,    45,     5,\n",
            "           271,     3,    68,  1400,     7,  9774,   932,    10,   102,     2,\n",
            "            20,   143,    28,    76,    55,  3810,     9,  2723,     5,    12,\n",
            "            10,   379,     2,  7372,    15,     4,    50,   710,     8,    13,\n",
            "            24,   887,    32,    31,    19,     8,    13,   428],\n",
            "        [18923,     7,     4,  4753,  1669,    12,  3019,     6,     4, 13906,\n",
            "           502,    40,    25,    77,  1588,     9,   115,     6, 21713,     2,\n",
            "            90,   305,   237,     9,   502,    33,    77,   376,     4, 16848,\n",
            "           847,    62,    77,   131,     9,     2,  1580,   338,     5, 18923,\n",
            "            32,     2,  1980,    49,   157,   306, 21713,    46,   981,     6,\n",
            "         10298,     2, 18924,   125,     9,   502,     3,   453,     4,  1852,\n",
            "           630,   407,  3407,    34,   277,    29,   242,     2, 20200,     5,\n",
            "         18923,    77,    95,    41,  1833,     6,  2105,    56,     3,   495,\n",
            "           214,   528,     2,  3479,     2,   112,     7,   181,  1813,     3,\n",
            "           597,     5,     2,   156,   294,     4,   543,   173,     9,  1562,\n",
            "           289, 10038,     5,     2,    20,    26,   841,  1392,    62,   130,\n",
            "           111,    72,   832,    26,   181, 12402,    15,    69,   183,     6,\n",
            "            66,    55,   936,     5,     2,    63,     8,     7,    43,     4,\n",
            "            78, 23726, 15995,    13,    20,    17,   800,     5,   392,    59,\n",
            "          3992,     3,   371,   103,  2596,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "tensor([1., 1., 1., 0.])\n",
            "tensor([165,  86, 218, 145])\n",
            "torch.Size([4, 218])\n"
          ]
        }
      ],
      "source": [
        "## ì‘ì€ ë°°ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
        "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
        "print(text_batch)\n",
        "print(label_batch)\n",
        "print(length_batch)\n",
        "print(text_batch.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "M4zDK9cbos6I"
      },
      "outputs": [],
      "source": [
        "## ë‹¨ê³„ 4: ë°ì´í„°ì…‹ ë°°ì¹˜ ë§Œë“¤ê¸°\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, collate_fn=collate_batch)\n",
        "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                      shuffle=False, collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                     shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rQtkeXjos6J"
      },
      "source": [
        "### ë¬¸ì¥ ì¸ì½”ë”©ì„ ìœ„í•œ ì„ë² ë”© ì¸µ\n",
        "\n",
        " * `input_dim`: ë‹¨ì–´ ìˆ˜, i.e. ì •ìˆ˜ ì¸ë±ìŠ¤ ìµœëŒ“ê°’ + 1.\n",
        " * `output_dim`:\n",
        " * `input_length`: (íŒ¨ë”©ëœ) ì‹œí€€ìŠ¤ ê¸¸ì´\n",
        "    * ì˜ˆë¥¼ ë“¤ì–´, `'This is an example' -> [0, 0, 0, 0, 0, 0, 3, 1, 8, 9]`   \n",
        "    => input_lenghtëŠ” 10\n",
        " * ì¸µì„ í˜¸ì¶œí•  ë•Œ ì •ìˆ˜ ê°’ì„ ì…ë ¥ìœ¼ë¡œ ë°›ê³  ì„ë² ë”© ì¸µì´ ì •ìˆ˜ë¥¼ `[output_dim]` í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë°”ê¿‰ë‹ˆë‹¤.\n",
        "   * ì…ë ¥ í¬ê¸°ê°€ `[BATCH_SIZE]`ì´ë©´, ì¶œë ¥ í¬ê¸°ëŠ” `[BATCH_SIZE, output_dim]`ê°€ ë©ë‹ˆë‹¤.\n",
        "   * ì…ë ¥ í¬ê¸°ê°€ `[BATCH_SIZE, 10]`ì´ë©´, ì¶œë ¥ í¬ê¸°ëŠ” `[BATCH_SIZE, 10, output_dim]`ê°€ ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì…ë ¥ ì‹œí€€ìŠ¤ : ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ ì…ë ¥ íŠ¹ì„±\n",
        "\n",
        "1) one-hot encoding\n",
        "\n",
        " - ì „ì²´ ë°ì´í„°ì„¸íŠ¸ì˜ ë‹¨ì–´ ìˆ˜ì— í•´ë‹¹í•˜ëŠ” í¬ê¸°ì˜ ë²¡í„°ë¥¼ ë§Œë“¬\n",
        "\n",
        "  > ì „ì²´ ë‹¨ì–´ ìˆ˜ê°€ ë§ìœ¼ë©´ ë²¡í„°ì˜ ì°¨ì›/íŠ¹ì„±ì´ ë‹¨ì–´ìˆ˜ > ì°¨ì›ì˜ ì €ì£¼\n",
        "\n",
        "2) ì„ë² ë”©\n",
        "\n",
        " - ê° ë‹¨ì–´ë¥¼ ì‹¤ìˆ˜ ê°’ì„ ê°–ëŠ” ê³ ì •ëœ ê¸¸ì´ì˜ ë²¡í„°ë¡œ ë³€í™˜\n",
        "\n",
        " - ì„ë² ë”©ì€ â€˜ë¯¸ë¦¬ ì •í•´ì£¼ëŠ” ê°’â€™ì´ ì•„ë‹ˆë¼, ì‹ ê²½ë§ì´ ì†ì‹¤(loss)ì„ ì¤„ì´ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ê²°ì •ë˜ëŠ” í•™ìŠµ íŒŒë¼ë¯¸í„°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì‹ ê²½ë§ë§Œìœ¼ë¡œ ì„ë² ë”©ì„ ë¶€ì—¬í•œë‹¤\n",
        "\n",
        "- ì„ë² ë”©ì€ ì…ë ¥ì´ì§€ë§Œ, ë™ì‹œì— í•™ìŠµë˜ëŠ” ê°€ì¤‘ì¹˜(parameter)\n",
        "\n",
        "- ì„ë² ë”© ë²¡í„°ë“¤ì´ LSTM / CNN / Linear ë“±ì„ í†µê³¼\n",
        "\n",
        "  > ì˜ˆì¸¡ê°’  ğ‘¦^  ìƒì„±\n",
        "\n",
        "  > ì†ì‹¤ê³„ì‚°: loss = L(y^,y)\n",
        "\n",
        "  > ì—­ì „íŒŒ(ì„ë² ë”© í•™ìŠµ): â€‹âˆ‚L/âˆ‚ew !=0 : ewëŠ” ë‹¨ì–´ wì— ëŒ€í•œ ì„ë² ë”©\n",
        "\n",
        "  > SGD: ew <- ew - m â€‹âˆ‚L/âˆ‚ew\n",
        "\n",
        "  > ì‹ ê²½ë§ í•™ìŠµìœ¼ë¡œ ì„ë² ë”© ê°’ì´ ë¶€ì—¬ëœë‹¤"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì„ë² ë”©ì„ ì‹ ê²½ë§ìœ¼ë¡œ í•™ìŠµí•œë‹¤ëŠ” ë§ì€ â€œë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ë¯¸ë¦¬ ì •í•´ë‘ëŠ” ê²ƒì´ ì•„ë‹ˆë¼,\n",
        "\n",
        "ì˜ˆì¸¡ì„ ì˜ í•˜ë„ë¡ ìˆ«ì(ë²¡í„°)ë¥¼ ì‹ ê²½ë§ í•™ìŠµìœ¼ë¡œ ê³„ì† ê³ ì³ ê°€ë©° ë°°ìš°ê²Œ í•œë‹¤â€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ë‹¨ì–´ ë¹ˆë„(BOW(Bag of Words), TF-IDF)\n",
        "\n",
        "movie â†’ [0,0,1,0,0,...]\n",
        "good  â†’ [0,1,0,0,0,...]\n",
        "\n",
        " - ì˜ë¯¸ë¥¼ ëª¨ë¦„\n",
        "\n",
        " - good â†” great â†” excellent ê´€ê³„ ì „í˜€ ëª¨ë¦„\n",
        "\n",
        " - ìˆœì„œÂ·ë¬¸ë§¥ ë¬´ì‹œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì‹ ê²½ë§ í•™ìŠµ - ì„ë² ë”©\n",
        "\n",
        "1. ë‹¨ì–´ â†’ ì„ë² ë”© ë²¡í„°\n",
        "\n",
        " \"movie\" â†’ e_movie = [0.3, -1.2, 0.7, ...]\n",
        "\n",
        " - ì²˜ìŒì—” ëœë¤ ê°’\n",
        " \n",
        "\n",
        "2. ì„ë² ë”©ì´ ì‹ ê²½ë§ì„ í†µê³¼\n",
        " \n",
        " - ì„ë² ë”© â†’ LSTM / CNN / Linear â†’ ì˜ˆì¸¡ê°’ Å·\n",
        "\n",
        " - y^: ê°ì„± ë¶„ë¥˜, ì£¼ì œ ë¶„ë¥˜, ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì— ì‚¬ìš©ë¨\n",
        "\n",
        "3. ì†ì‹¤ ê³„ì‚°\n",
        "\n",
        " - loss = L(Å·, y)\n",
        "\n",
        " - ì—­ì „íŒŒê°€ â€œì„ë² ë”©ê¹Œì§€â€ ë‚´ë ¤ì˜¨ë‹¤: dL/dew != 0\n",
        "\n",
        "4. SGDë¡œ ì„ë² ë”© ìì²´ë¥¼ ìˆ˜ì •\n",
        "\n",
        " - ew = ew - mu . dL/dew > ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ì—…ë°ì´íŠ¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ê°ì„± ë¶„ë¥˜: â€œgreatâ€, â€œexcellentâ€ê°€ ìì£¼ **ê°™ì€ ë¼ë²¨(ê¸ì •)**ì— ê¸°ì—¬\n",
        "\n",
        "- ëª¨ë¸ì€ lossë¥¼ ì¤„ì´ê¸° ìœ„í•´\n",
        "\n",
        "- ì´ ë‹¨ì–´ë“¤ì˜ ì„ë² ë”©ì„ ë¹„ìŠ·í•œ ë°©í–¥ìœ¼ë¡œ ì´ë™"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"good\" â‰ˆ [ 1.3,  0.9]\n",
        "\"great\"â‰ˆ [ 1.2,  1.0]\n",
        "\n",
        "\"bad\"  â‰ˆ [-1.1, -0.8]\n",
        "\"awful\"â‰ˆ [-1.2, -0.9]\n",
        "\n",
        "- ì°¨ì› í•˜ë‚˜í•˜ë‚˜ëŠ” ì˜ë¯¸ ì—†ìŒ\n",
        "\n",
        " > ë²¡í„°ì˜ ë°©í–¥Â·ê±°ë¦¬ê°€ ì˜ë¯¸\n",
        "\n",
        " > ì„ë² ë”©ì€ ì‹ ê²½ë§ í•™ìŠµ ê³¼ì •ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë„ë¡\n",
        "\n",
        "  + ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë“¤ê³¼ ë™ì¼í•˜ê²Œ ì—­ì „íŒŒë¡œ ì—…ë°ì´íŠ¸ë˜ë©°,\n",
        "\n",
        "  + ê·¸ ê²°ê³¼ ê³¼ì œ ìˆ˜í–‰ì— ìœ ìš©í•œ ì˜ë¯¸ì  íŠ¹ì„±ì´ ë²¡í„° ê³µê°„ì— ë°˜ì˜ëœë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "n0UA30mqos6J",
        "outputId": "5ed72b98-e1bf-4852-9525-da07d65af814"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_10.png\" width=\"600\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_10.png', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "['i', 'really', 'like', 'this']\n",
        "â†’ [1, 2, 4, 5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLLGq2a5os6J",
        "outputId": "c0a42f4f-1bbc-435f-d716-0215eedf7ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4651, -0.3203,  2.2408],\n",
            "         [ 0.3824, -0.3446, -0.3531],\n",
            "         [-0.0251, -0.5973, -0.2959],\n",
            "         [ 0.8356,  0.4025, -0.6924]],\n",
            "\n",
            "        [[-0.0251, -0.5973, -0.2959],\n",
            "         [ 0.9124, -0.4643,  0.3046],\n",
            "         [ 0.3824, -0.3446, -0.3531],\n",
            "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)\n"
          ]
        }
      ],
      "source": [
        "embedding = nn.Embedding(num_embeddings=10, # ì„ë² ë”© í–‰ì˜ ìˆ˜ > ë‹¨ì–´ ìˆ˜\n",
        "                         embedding_dim=3,\n",
        "                         padding_idx=0)\n",
        "\n",
        "# ë„¤ ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì§„ ë‘ ê°œì˜ ìƒ˜í”Œë¡œ êµ¬ì„±ëœ ë°°ì¹˜\n",
        "text_encoded_input = torch.LongTensor([[1,2,4,5],[4,3,2,0]]) # ë‹¨ì–´ë“¤ì˜ ì •ìˆ˜ ì¸ì½”ë”©\n",
        "print(embedding(text_encoded_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIOilB2Qos6J"
      },
      "source": [
        "### RNN ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "\n",
        "* **RNN layers:**\n",
        "  * `nn.RNN(input_size, hidden_size, num_layers=1)`\n",
        "  * `nn.LSTM(..)`\n",
        "  * `nn.GRU(..)`\n",
        "  * `nn.RNN(input_size, hidden_size, num_layers=1, bidirectional=True)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jeVvyySos6J",
        "outputId": "09ca642f-bb24-4d89-f77b-724d09a263a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.2565],\n",
              "        [ 0.0027],\n",
              "        [-0.0861],\n",
              "        [ 0.2367],\n",
              "        [-0.1699]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## ê°„ë‹¨í•œ RNN ì¸µì„ ì‚¬ìš©í•œ RNN ëª¨ë¸ êµ¬ì¶• ì˜ˆì œ\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size,\n",
        "                          hidden_size,\n",
        "                          num_layers=2,\n",
        "                          batch_first=True)\n",
        "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1) # ì¶œë ¥ì¸µ\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, hidden = self.rnn(x)\n",
        "        out = hidden[-1, :, :] # hidden[-1] â†’ 2ì¸µ ì¤‘ ë§ˆì§€ë§‰ ì¸µì˜ ì€ë‹‰ ìƒíƒœ\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = RNN(64, 32)\n",
        "\n",
        "print(model)\n",
        "\n",
        "model(torch.randn(5, 3, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ì™œ many-to-one ë¬¸ì œì—ì„œëŠ” hidden[-1]ì¸ê°€?\n",
        "\n",
        "- ì…ë ¥: ì‹œí€€ìŠ¤ ì „ì²´ (ë¬¸ì¥, ì‹œê³„ì—´)\n",
        "\n",
        "- ì¶œë ¥: í•˜ë‚˜ì˜ ê°’\n",
        "\n",
        " > ê°ì„± ë¶„ë¥˜\n",
        "\n",
        " > ë¬¸ì¥ ë¶„ë¥˜\n",
        "\n",
        " > ì‹œê³„ì—´ ì˜ˆì¸¡ 1ê°œ ê°’"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RNN ë‚´ë¶€ ê³„ì‚°:\n",
        "\n",
        " ht = f(ht-1,xt)\n",
        "\n",
        " ë§ˆì§€ë§‰ ì‹œì : hT = f(hT-1, xT) > hTì—ëŠ” x1 ~ xTì˜ ì •ë³´ê°€ ëˆ„ì ë˜ì–´ ìˆë‹¤ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "LSTMì—ëŠ” ì—­í• ì´ ì™„ì „íˆ ë‹¤ë¥¸ ë‘ ì¢…ë¥˜ì˜ ê°’ì´ ìˆë‹¤.\n",
        "\n",
        "- ë¬´ì—‡ì„ ê¸°ì–µ/í‘œí˜„: ct, ht > cell ìƒíƒœë¡œ í‘œí˜„\n",
        "\n",
        "  ct = ft.ct-1 + it.ct~\n",
        "\n",
        "  ht = ot.tanh(ct)\n",
        "\n",
        "- ì–¼ë§ˆë‚˜ ìœ ì§€/ì¶”ê°€/ë…¸ì¶œí•  ê²ƒì¸ê°€ë¥¼ ì¡°ì ˆ: ft, it, ot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jor6xakUos6J"
      },
      "source": [
        "### ê°ì„± ë¶„ì„ ì‘ì—…ì„ ìœ„í•œ RNN ëª¨ë¸ ë§Œë“¤ê¸°\n",
        "\n",
        "- ê°€ë³€ ê¸¸ì´ í…ìŠ¤íŠ¸(íŒ¨ë”© í¬í•¨)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ LSTMìœ¼ë¡œ ë¬¸ì¥(ë¦¬ë·°) ì „ì²´ë¥¼ ìš”ì•½\n",
        "\n",
        "- ì´ì§„ ë¶„ë¥˜(ê¸/ë¶€ì •) í™•ë¥ ì„ ì¶œë ¥í•˜ëŠ” many-to-one ëª¨ë¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0Y9p4CVuos6J"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size,\n",
        "                                      embed_dim,\n",
        "                                      padding_idx=0)\n",
        "        # ì…ë ¥ì€ textê°€ ì •ìˆ˜ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤(ì˜ˆ: [12, 53, 7, ...])\n",
        "        # nn.Embeddingì€ ì´ë¥¼ ì‹¤ìˆ˜ ë²¡í„° ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        # LSTMì´ ë§Œë“  â€œë¬¸ì¥ ìš”ì•½ ë²¡í„°â€ë¥¼ ë°›ëŠ”ë‹¤ \n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text) #text: íŒ¨ë”©ëœ ë¬¸ì¥ ì¸ë±ìŠ¤\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
        "        # ê° ë¬¸ì¥ë§ˆë‹¤ ì‹¤ì œ ê¸¸ì´ê¹Œì§€ë§Œ RNNì´ ì²˜ë¦¬\n",
        "        # PAD ë¶€ë¶„ì€ ì•„ì˜ˆ ê³„ì‚°ì—ì„œ ì œì™¸ë¨(íš¨ìœ¨ + ì •í™•ì„±)\n",
        "\n",
        "        out, (hidden, cell) = self.rnn(out)\n",
        "        # outì€ ê° ì‹œì ë“¤ì˜ htë¥¼ ëª¨ì€ ê²ƒ \n",
        "        # hidden[-1] : ë§ˆì§€ë§‰ ì‹œì  ìš”ì•½ â†’ í•„ìš”\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 20\n",
        "rnn_hidden_size = 64\n",
        "fc_hidden_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCF7f6p9os6J"
      },
      "outputs": [],
      "source": [
        "def train(dataloader):\n",
        "    # í•™ìŠµìš© DataLoaderë¥¼ ë°›ì•„ì„œ 1 epoch í•™ìŠµì„ ìˆ˜í–‰í•˜ê³ \n",
        "    # ì •í™•ë„(acc), í‰ê·  ì†ì‹¤(loss) ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜\n",
        "    model.train()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    for text_batch, label_batch, lengths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text_batch, lengths)[:, 0]\n",
        "        loss = loss_fn(pred, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
        "        total_loss += loss.item()*label_batch.size(0)\n",
        "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch, lengths in dataloader:\n",
        "            pred = model(text_batch, lengths)[:, 0]\n",
        "            loss = loss_fn(pred, label_batch)\n",
        "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
        "            total_loss += loss.item()*label_batch.size(0)\n",
        "    return total_acc/len(list(dataloader.dataset)), total_loss/len(list(dataloader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwr5p12bos6K",
        "outputId": "38b7919a-92c2-49b1-cfa1-21bb2f04e236"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m----> 9\u001b[0m     acc_train, loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     acc_valid, loss_valid \u001b[38;5;241m=\u001b[39m evaluate(valid_dl)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mì—í¬í¬ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ì •í™•ë„: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_train\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ê²€ì¦ ì •í™•ë„: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_valid\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[18], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      6\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(text_batch, lengths)[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, label_batch)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m total_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((pred\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m==\u001b[39m label_batch)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\_tensor.py:255\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    248\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    249\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    254\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[1;32m--> 255\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\venv39\\lib\\site-packages\\torch\\autograd\\__init__.py:147\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m--> 147\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train(train_dl)\n",
        "    acc_valid, loss_valid = evaluate(valid_dl)\n",
        "    print(f'ì—í¬í¬ {epoch} ì •í™•ë„: {acc_train:.4f} ê²€ì¦ ì •í™•ë„: {acc_valid:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoVkqwdaos6K",
        "outputId": "d47c1bf1-c3a2-4eb0-ff1f-afd669890c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.8494\n"
          ]
        }
      ],
      "source": [
        "acc_test, _ = evaluate(test_dl)\n",
        "print(f'í…ŒìŠ¤íŠ¸ ì •í™•ë„: {acc_test:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlBVUcdKos6K"
      },
      "source": [
        "#### ì–‘ë°©í–¥ RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XygmgeM9os6K"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size,\n",
        "                                      embed_dim,\n",
        "                                      padding_idx=0)\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True, bidirectional=True)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size*2, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text)\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False, batch_first=True)\n",
        "        _, (hidden, cell) = self.rnn(out)\n",
        "        out = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWefjEY1os6K",
        "outputId": "f54cd7de-5f3a-491a-9461-250d5bf899aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ì—í¬í¬ 0 ì •í™•ë„: 0.6254 ê²€ì¦ ì •í™•ë„: 0.7030\n",
            "ì—í¬í¬ 1 ì •í™•ë„: 0.7802 ê²€ì¦ ì •í™•ë„: 0.7444\n",
            "ì—í¬í¬ 2 ì •í™•ë„: 0.8495 ê²€ì¦ ì •í™•ë„: 0.8462\n",
            "ì—í¬í¬ 3 ì •í™•ë„: 0.8912 ê²€ì¦ ì •í™•ë„: 0.8384\n",
            "ì—í¬í¬ 4 ì •í™•ë„: 0.9315 ê²€ì¦ ì •í™•ë„: 0.8546\n",
            "ì—í¬í¬ 5 ì •í™•ë„: 0.9522 ê²€ì¦ ì •í™•ë„: 0.8546\n",
            "ì—í¬í¬ 6 ì •í™•ë„: 0.9700 ê²€ì¦ ì •í™•ë„: 0.8700\n",
            "ì—í¬í¬ 7 ì •í™•ë„: 0.9792 ê²€ì¦ ì •í™•ë„: 0.8448\n",
            "ì—í¬í¬ 8 ì •í™•ë„: 0.9857 ê²€ì¦ ì •í™•ë„: 0.8668\n",
            "ì—í¬í¬ 9 ì •í™•ë„: 0.9943 ê²€ì¦ ì •í™•ë„: 0.8664\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train(train_dl)\n",
        "    acc_valid, loss_valid = evaluate(valid_dl)\n",
        "    print(f'ì—í¬í¬ {epoch} ì •í™•ë„: {acc_train:.4f} ê²€ì¦ ì •í™•ë„: {acc_valid:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "LJDBchIeos6K"
      },
      "outputs": [],
      "source": [
        "test_dataset = IMDB(split='test')\n",
        "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                     shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QE9J_knos6K",
        "outputId": "1d25efc0-9256-4441-8ffe-10e9a9e58c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "í…ŒìŠ¤íŠ¸ ì •í™•ë„: 0.8480\n"
          ]
        }
      ],
      "source": [
        "acc_test, _ = evaluate(test_dl)\n",
        "print(f'í…ŒìŠ¤íŠ¸ ì •í™•ë„: {acc_test:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
