{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b95ea5",
   "metadata": {},
   "source": [
    "## 트랜스포머\n",
    "- RNN의 속성을 따라감 + 트랜스포머\n",
    "- 어텐션 메커니즘\n",
    "    - 멀티 헤드 어텐션으로 문맥 임베딩 인코딩하기\n",
    "    - 디코더와 마스크드 멀티 헤드 어텐션\n",
    "    - 위치 인코딩 및 층 정규화\n",
    "\n",
    "- K, Q, V\n",
    "    - Q * K (내적, dot product) => 두 벡터의 방향 유사도\n",
    "        - 병렬 처리에 \"약간\" 더 좋음\n",
    "        - Q(질문), K(키)가 각각의 벡터에서 얼마나 가까운가?\n",
    "        - \"이 Q(쿼리)가 각 K(토큰)에 얼마나 집중(주목)해야 하나요?\"\n",
    "\n",
    "    - softmax(내적 값): 가중치 정규화\n",
    "    - 가중치 × V → 최종 표현\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets accelerate evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35755380",
   "metadata": {},
   "source": [
    "## Chat형도\n",
    "\n",
    "> 일반 텍스트 -> 기형도 시인의 문체를 약간이나마 따라하는 출력 결과를 보고 싶음\n",
    "\n",
    "### 설계\n",
    "```Python\n",
    "1. 기존에 학습된 모델이 필요함(Base 모델)\n",
    "2. NLP의 경우 tokenizer를 결정, 입력 데이터에 따라 숫자로 변환하는 알고리즘\n",
    "3. 임베딩을 진행, 숫자로 변환\n",
    "4. 데이터를 Base 모델에 학습(파인 튜닝)\n",
    "5. 평가를 진행(샘플 텍스트를 몇 개 작성)\n",
    "6. 확인(그래프, 숫자, 표)\n",
    "7. 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBARTFineTuningConfig:\n",
    "    # 모델 설정\n",
    "    MODEL_NAME = \"gogamza/kobart-base-v2\"  # 한국어 BART 모델\n",
    "\n",
    "    # 훈련 하이퍼파라미터\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WARMUP_STEPS = 500\n",
    "    MAX_SOURCE_LENGTH = 512  # 입력 텍스트 최대 길이\n",
    "    MAX_TARGET_LENGTH = 128  # 출력 텍스트 최대 길이\n",
    "\n",
    "    # 출력 설정\n",
    "    OUTPUT_DIR = \"./kobart-finetuned\"\n",
    "    LOGGING_DIR = \"./logs\"\n",
    "\n",
    "    # 평가 설정\n",
    "    EVAL_STEPS = 500\n",
    "    SAVE_STEPS = 500\n",
    "    LOGGING_STEPS = 100\n",
    "\n",
    "    # 생성 설정 (추론 시)\n",
    "    NUM_BEAMS = 4\n",
    "    MAX_GENERATION_LENGTH = 128\n",
    "    EARLY_STOPPING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af09fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471e45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 23\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. 데이터셋 준비 및 전처리\n",
    "# ============================================================================\n",
    "def load_custom_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            split: Dataset.from_list(data[split])\n",
    "            for split in [\"train\", \"validation\", \"test\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. 토크나이저 및 모델 로드\n",
    "# ============================================================================\n",
    "def load_tokenizer_and_model(model_name=None):\n",
    "    if model_name is None:\n",
    "        model_name = KoBARTFineTuningConfig.MODEL_NAME\n",
    "\n",
    "    print(f\"모델 로딩 중: {model_name}\")\n",
    "\n",
    "    # 토크나이저 로드\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "    # 모델 로드\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    print(f\"토크나이저 어휘 크기: {len(tokenizer)}\")\n",
    "    print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103499a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. 데이터 전처리 함수\n",
    "# ============================================================================\n",
    "def preprocess_function(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    max_source_length,\n",
    "    max_target_length,\n",
    "):\n",
    "\n",
    "    # 입력 텍스트 (document) 토큰화\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"document\"],\n",
    "        max_length=max_source_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_token_type_ids=False,  # BART는 token_type_ids를 사용하지 않음\n",
    "    )\n",
    "\n",
    "    # 타겟 텍스트 (summary) 토큰화\n",
    "    # 최신 transformers에서는 text_target 인자 사용 (as_target_tokenizer 대신)\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # labels를 model_inputs에 추가\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # token_type_ids가 있다면 제거 (BART는 사용하지 않음)\n",
    "    if \"token_type_ids\" in model_inputs:\n",
    "        del model_inputs[\"token_type_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, tokenizer):\n",
    "    config = KoBARTFineTuningConfig\n",
    "\n",
    "    # 각 split에 대해 전처리 적용\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda examples: preprocess_function(\n",
    "            examples, tokenizer, config.MAX_SOURCE_LENGTH, config.MAX_TARGET_LENGTH\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,  # 원본 컬럼 제거\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b442a5cf",
   "metadata": {},
   "source": [
    "### 학습\n",
    "```Python\n",
    "1. 데이터셋 준비\n",
    "2. 토크나이저 및 모델\n",
    "3. 데이터 전처리\n",
    "4. 데이터 폴레이터\n",
    "5. 데이터 파라미터 결정\n",
    "6. 훈련\n",
    "7. 평가\n",
    "8. 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de1771c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 장치: cpu\n",
      "  훈련 데이터: 23개\n",
      "  검증 데이터: 4개\n",
      "  테스트 데이터: 6개\n",
      "모델 로딩 중: gogamza/kobart-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 어휘 크기: 30000\n",
      "모델 파라미터 수: 123,859,968\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50fb442a4124509b9c303e2f191c6d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d941ed0402c445479a727021bfc9d608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f70db64c620409e97278d918e75177c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[7단계] 훈련 시작...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\torch2\\Lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\miniconda3\\envs\\torch2\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "테스트 세트 평가 결과:\n",
      "  eval_loss: 22.8824\n",
      "  eval_rouge1: 0.0000\n",
      "  eval_rouge2: 0.0000\n",
      "  eval_rougeL: 0.0000\n",
      "  eval_rougeLsum: 0.0000\n",
      "  eval_runtime: 31.4575\n",
      "  eval_samples_per_second: 0.1910\n",
      "  eval_steps_per_second: 0.0320\n",
      "  epoch: 3.0000\n",
      "\n",
      "[9단계] 모델 저장 중: ./kobart-finetuned\n",
      "\n",
      "파인튜닝 완료!\n",
      "\n",
      "============================================================\n",
      "추론 테스트\n",
      "============================================================\n",
      "\n",
      "원본 텍스트:\n",
      "아침 저녁으로 샛강에 자욱이 안개가 낀다. 이 읍에 처음 와본 사람은 누구나 거대한 안개의 강을 거쳐야 한다. 앞서간 일행들이 천천히 지워질 때까지 쓸쓸한 가축들처럼 그들은 그 긴 방죽 위에 서 있어야 한다. 문득 저 홀로 안개의 빈 구멍 속에 갇혀 있음을 느끼고 경악할 때까지.\n",
      "\n",
      "생성된 요약:\n",
      "일들들들이 천천히 지워질 때까지 쓸쓸한 가축들처럼 그들은 그 긴 방죽 위에 서 있어야 한다. 문득 저 홀로 안개의 빈 구멍 속에 갇혀 있음을 느끼고 경악할 때까지. 문득. 지나 지나 샛 강에 자욱이 안개가 낀다. 이 읍은 아침 저녁으로 달아   은 은 안위가 안무가 껴다. 자 선 선은 안이가 안트가  끼다.이 읍에 처음 와본 사람은 누구나 거대한 큰 큰 안욱 이 안대가  끼고다. 나 읍 에 처음 나 나 이 이 지난 지난 다 다 지나 남 남들 같은 이렇게 이렇게 그들은\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. 설정 및 초기화\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class KoBARTFineTuningConfig:\n",
    "    # 모델 설정\n",
    "    MODEL_NAME = \"gogamza/kobart-base-v2\"  # 한국어 BART 모델\n",
    "\n",
    "    # 훈련 하이퍼파라미터\n",
    "    NUM_EPOCHS = 3\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 2e-5\n",
    "    WARMUP_STEPS = 500\n",
    "    MAX_SOURCE_LENGTH = 512  # 입력 텍스트 최대 길이\n",
    "    MAX_TARGET_LENGTH = 128  # 출력 텍스트 최대 길이\n",
    "\n",
    "    # 출력 설정\n",
    "    OUTPUT_DIR = \"./kobart-finetuned\"\n",
    "    LOGGING_DIR = \"./logs\"\n",
    "\n",
    "    # 평가 설정\n",
    "    EVAL_STEPS = 500\n",
    "    SAVE_STEPS = 500\n",
    "    LOGGING_STEPS = 100\n",
    "\n",
    "    # 생성 설정 (추론 시)\n",
    "    NUM_BEAMS = 4\n",
    "    MAX_GENERATION_LENGTH = 128\n",
    "    EARLY_STOPPING = True\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. 데이터셋 준비 및 전처리\n",
    "# ============================================================================\n",
    "def load_custom_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dataset_dict = DatasetDict(\n",
    "        {\n",
    "            split: Dataset.from_list(data[split])\n",
    "            for split in [\"train\", \"validation\", \"test\"]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. 토크나이저 및 모델 로드\n",
    "# ============================================================================\n",
    "def load_tokenizer_and_model(model_name=None):\n",
    "    if model_name is None:\n",
    "        model_name = KoBARTFineTuningConfig.MODEL_NAME\n",
    "\n",
    "    print(f\"모델 로딩 중: {model_name}\")\n",
    "\n",
    "    # 토크나이저 로드\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "    # 모델 로드\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    print(f\"토크나이저 어휘 크기: {len(tokenizer)}\")\n",
    "    print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. 데이터 전처리 함수\n",
    "# ============================================================================\n",
    "def preprocess_function(\n",
    "    examples,\n",
    "    tokenizer,\n",
    "    max_source_length,\n",
    "    max_target_length,\n",
    "):\n",
    "\n",
    "    # 입력 텍스트 (document) 토큰화\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"document\"],\n",
    "        max_length=max_source_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_token_type_ids=False,  # BART는 token_type_ids를 사용하지 않음\n",
    "    )\n",
    "\n",
    "    # 타겟 텍스트 (summary) 토큰화\n",
    "    # 최신 transformers에서는 text_target 인자 사용 (as_target_tokenizer 대신)\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"summary\"],\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # labels를 model_inputs에 추가\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # token_type_ids가 있다면 제거 (BART는 사용하지 않음)\n",
    "    if \"token_type_ids\" in model_inputs:\n",
    "        del model_inputs[\"token_type_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, tokenizer):\n",
    "    config = KoBARTFineTuningConfig\n",
    "\n",
    "    # 각 split에 대해 전처리 적용\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda examples: preprocess_function(\n",
    "            examples, tokenizer, config.MAX_SOURCE_LENGTH, config.MAX_TARGET_LENGTH\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names,  # 원본 컬럼 제거\n",
    "    )\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. 평가 메트릭 설정\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    # ROUGE 메트릭 로드\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # 디코딩 (패딩 토큰 제거)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # labels에서 -100 (무시할 토큰)을 pad_token_id로 변환\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGE 점수 계산\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "\n",
    "    # 점수를 백분율로 변환\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. 훈련 설정 및 실행\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def setup_training_args(output_dir=None):\n",
    "    config = KoBARTFineTuningConfig\n",
    "\n",
    "    if output_dir is None:\n",
    "        output_dir = config.OUTPUT_DIR\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        # 출력 설정\n",
    "        output_dir=output_dir,\n",
    "        logging_dir=config.LOGGING_DIR,\n",
    "        # 훈련 설정\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        warmup_steps=config.WARMUP_STEPS,\n",
    "        # 평가 및 저장 설정\n",
    "        eval_strategy=\"steps\",  # 최신 transformers에서는 eval_strategy 사용\n",
    "        eval_steps=config.EVAL_STEPS,\n",
    "        save_strategy=\"steps\",  # 저장 전략도 명시적으로 설정\n",
    "        save_steps=config.SAVE_STEPS,\n",
    "        logging_steps=config.LOGGING_STEPS,\n",
    "        # 최적화 설정\n",
    "        gradient_accumulation_steps=2,\n",
    "        fp16=torch.cuda.is_available(),  # GPU가 있으면 mixed precision 사용\n",
    "        # MPS(Metal) 환경에서는 multiprocessing이 제대로 작동하지 않으므로 0으로 설정\n",
    "        dataloader_num_workers=0 if torch.backends.mps.is_available() else 4,\n",
    "        # MPS 환경에서는 pin_memory를 비활성화하여 경고 제거\n",
    "        dataloader_pin_memory=False if torch.backends.mps.is_available() else True,\n",
    "        # 모델 저장 설정\n",
    "        save_total_limit=3,  # 최대 3개의 체크포인트만 유지\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge1\",  # ROUGE-1 점수를 기준으로 최적 모델 선택\n",
    "        # 생성 설정\n",
    "        predict_with_generate=True,  # 평가 시 생성된 텍스트로 메트릭 계산\n",
    "        # 기타 설정\n",
    "        report_to=\"tensorboard\",  # TensorBoard 로깅\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def train_kobart(\n",
    "    dataset=None,\n",
    "    model_name=None,\n",
    "    output_dir=None,\n",
    "    use_sample_data=True,\n",
    "    json_file=None,\n",
    "):\n",
    "    if dataset is None:\n",
    "        dataset = load_custom_dataset(json_file)\n",
    "\n",
    "    print(f\"  훈련 데이터: {len(dataset['train'])}개\")\n",
    "    print(f\"  검증 데이터: {len(dataset['validation'])}개\")\n",
    "    print(f\"  테스트 데이터: {len(dataset['test'])}개\")\n",
    "\n",
    "    # 2. 토크나이저 및 모델 로드\n",
    "    tokenizer, model = load_tokenizer_and_model(model_name)\n",
    "\n",
    "    # 3. 데이터 전처리\n",
    "    tokenized_dataset = prepare_dataset(dataset, tokenizer)\n",
    "\n",
    "    # 4. 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer, model=model, padding=True\n",
    "    )\n",
    "\n",
    "    # 5. 훈련 인자 설정\n",
    "    training_args = setup_training_args(output_dir)\n",
    "\n",
    "    # 6. Trainer 생성\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        processing_class=tokenizer,  # 최신 transformers에서는 processing_class 사용\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "    )\n",
    "\n",
    "    # 7. 훈련 실행\n",
    "    print(\"\\n[7단계] 훈련 시작...\")\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # 8. 최종 평가\n",
    "    eval_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "    print(f\"\\n테스트 세트 평가 결과:\")\n",
    "    for key, value in eval_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "    # 9. 모델 저장\n",
    "    print(f\"\\n[9단계] 모델 저장 중: {training_args.output_dir}\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "\n",
    "    print(\"\\n파인튜닝 완료!\")\n",
    "\n",
    "    return trainer, tokenizer, model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. 추론 함수\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def generate_summary(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text,\n",
    "    max_length=None,\n",
    "    num_beams=None,\n",
    "):\n",
    "    config = KoBARTFineTuningConfig\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = config.MAX_TARGET_LENGTH\n",
    "    if num_beams is None:\n",
    "        num_beams = config.NUM_BEAMS\n",
    "\n",
    "    # 입력 토큰화\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=config.MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,  # BART는 token_type_ids를 사용하지 않음\n",
    "    )\n",
    "\n",
    "    # GPU로 이동\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # token_type_ids가 있다면 제거 (BART는 사용하지 않음)\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "\n",
    "    # 요약 생성\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=config.EARLY_STOPPING,\n",
    "            no_repeat_ngram_size=2,  # 반복 방지\n",
    "            repetition_penalty=1.2,  # 반복 페널티\n",
    "        )\n",
    "\n",
    "    # 디코딩\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def load_finetuned_model(model_path):\n",
    "    print(f\"파인튜닝된 모델 로드 중: {model_path}\")\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. 메인 실행 함수\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 랜덤 시드 설정\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # GPU 사용 가능 여부 확인\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"사용 장치: {device}\")\n",
    "\n",
    "    # JSON 파일 경로 설정\n",
    "    json_file = \"data/기형도-시집.json\"\n",
    "\n",
    "    # 파인튜닝 실행 (JSON 파일 사용)\n",
    "    trainer, tokenizer, model = train_kobart(json_file=json_file, use_sample_data=False)\n",
    "\n",
    "    # 추론 테스트\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"추론 테스트\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 시집에서 샘플 텍스트 사용\n",
    "    test_text = (\n",
    "        \"아침 저녁으로 샛강에 자욱이 안개가 낀다. \"\n",
    "        \"이 읍에 처음 와본 사람은 누구나 거대한 안개의 강을 거쳐야 한다. \"\n",
    "        \"앞서간 일행들이 천천히 지워질 때까지 쓸쓸한 가축들처럼 그들은 \"\n",
    "        \"그 긴 방죽 위에 서 있어야 한다. 문득 저 홀로 안개의 빈 구멍 속에 \"\n",
    "        \"갇혀 있음을 느끼고 경악할 때까지.\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\n원본 텍스트:\\n{test_text}\\n\")\n",
    "\n",
    "    summary = generate_summary(model, tokenizer, test_text)\n",
    "    print(f\"생성된 요약:\\n{summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df061743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting rouge_score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: evaluate in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (0.4.6)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from rouge_score) (2.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from evaluate) (4.5.0)\n",
      "Requirement already satisfied: dill in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from evaluate) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from evaluate) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.20.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from datasets>=2.0.0->evaluate) (23.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from pandas->evaluate) (2025.3)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (pyproject.toml): started\n",
      "  Building wheel for rouge_score (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=abb533ab2776ac4b58363e55d1140b09489f514c7b91632819d180a0114b3391\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\44\\af\\da\\5ffc433e2786f0b1a9c6f458d5fb8f611d8eb332387f18698f\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: click, absl-py, nltk, rouge_score\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [absl-py]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   -------------------- ------------------- 2/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [rouge_score]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 click-8.3.1 nltk-3.9.2 rouge_score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U nltk rouge_score absl-py evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d4c5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from tensorboardX) (2.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\miniconda3\\envs\\torch2\\lib\\site-packages (from tensorboardX) (25.0)\n",
      "Collecting protobuf>=3.20 (from tensorboardX)\n",
      "  Downloading protobuf-6.33.4-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Downloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
      "Downloading protobuf-6.33.4-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Installing collected packages: protobuf, tensorboardX\n",
      "\n",
      "   ---------------------------------------- 0/2 [protobuf]\n",
      "   -------------------- ------------------- 1/2 [tensorboardX]\n",
      "   ---------------------------------------- 2/2 [tensorboardX]\n",
      "\n",
      "Successfully installed protobuf-6.33.4 tensorboardX-2.6.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorboardX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
